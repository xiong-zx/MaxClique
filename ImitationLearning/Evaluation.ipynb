{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1lu6Y2vRSRU3eWfUSQmElFPmtMz7kSDqD","timestamp":1702085455525}],"machine_shape":"hm","mount_file_id":"1lu6Y2vRSRU3eWfUSQmElFPmtMz7kSDqD","authorship_tag":"ABX9TyNzU/YCjBQf+XdoU2zUbKuw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"7bmp5VqpYkjY","executionInfo":{"status":"ok","timestamp":1702869734154,"user_tz":480,"elapsed":13696,"user":{"displayName":"Jia He Sun","userId":"07783893916712904513"}},"outputId":"95bfb311-211a-4b5d-c6e2-555ed5c1f857","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install torch torchvision torchaudio\n","!pip install torch-geometric"],"metadata":{"id":"zDwFkYkpYjM3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702869906765,"user_tz":480,"elapsed":14835,"user":{"displayName":"Jia He Sun","userId":"07783893916712904513"}},"outputId":"9246a721-bfe2-4a34-f6a7-a77b436cb730"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Collecting torch-geometric\n","  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.23.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2023.11.17)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.2.0)\n","Installing collected packages: torch-geometric\n","Successfully installed torch-geometric-2.4.0\n"]}]},{"cell_type":"code","source":["import networkx as nx\n","import matplotlib.pyplot as plt"],"metadata":{"id":"ELqcbhk-ExS8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# HELPER FUNCTIONS"],"metadata":{"id":"dBPhiLFbFhd1"}},{"cell_type":"code","source":["import networkx as nx\n","import random\n","import numpy\n","from torch_geometric.data import Data\n","\n","\n","def mc_upper_bound(G):\n","\t\"\"\"\n","\tINPUT:\n","\t - \"G\" Networkx Undirected Graph\n","\tOUTPUT:\n","\t - \"chromatic_number\" integer upper bound on the maximum clique number\n","\t\"\"\"\n","\tansw = nx.algorithms.coloring.greedy_color(G)\n","\tchromatic_number = list(set(list(answ.values())))\n","\treturn len(chromatic_number)\n","\n","def mc_lower_bound(G):\n","\t\"\"\"\n","\tINPUT:\n","\t - \"G\" Networkx Undirected Graph\n","\tOUTPUT:\n","\t - \"lower bound\" list of variables which form a clique in G\n","\t\"\"\"\n","\treturn nx.maximal_independent_set(nx.complement(G))\n","\n","def edge_k_core(G, k):\n","\t\"\"\"\n","\tINPUT:\n","\t - \"G\" Networkx Undirected Graph\n","\t - \"k\" Integer that is at least one less than the global maximum clique number\n","\tOUTPUT:\n","\t - \"G\" Networkx Undirected Graph where edge k-core reduction has been applied\n","\t\"\"\"\n","\tfor a in list(G.edges()):\n","\t\tx = list(G.neighbors(a[0]))\n","\t\ty = list(G.neighbors(a[1]))\n","\t\tif len(list(set(x) & set(y))) <= (k-2):\n","\t\t\tG.remove_edge(a[0], a[1])\n","\treturn G\n","\n","def k_core_reduction(graph, k):\n","\t\"\"\"\n","\tINPUT:\n","\t - \"graph\" Networkx Undirected Graph\n","\t - \"k\" Integer that is at least one less than the global maximum clique number\n","\tOUTPUT:\n","\t - \"graph\" Networkx Undirected Graph where k-core reduction has been applied\n","\t\"\"\"\n","\tgraph = nx.k_core(graph, k)\n","\tref1 = len(list(graph.edges()))\n","\tgraph = edge_k_core(graph, k)\n","\tref2 = len(list(graph.edges()))\n","\twhile ref1 != ref2:\n","\t\tif len(graph) == 0:\n","\t\t\treturn graph\n","\t\tgraph = nx.k_core(graph, k)\n","\t\tref1 = len(list(graph.edges()))\n","\t\tgraph = edge_k_core(graph, k)\n","\t\tref2 = len(list(graph.edges()))\n","\treturn graph\n","\n","def is_clique(G):\n","\t\"\"\"\n","\tINPUT:\n","\t - \"G\" Networkx Undirected Graph\n","\tOUTPUT:\n","\t - \"True\" if G is a clique, and \"False\" if G is not a clique\n","\t\"\"\"\n","\tn = len(list(G.nodes()))\n","\tm = len(list(G.edges()))\n","\tif int(m) == int((n*(n-1))/float(2)):\n","\t\treturn True\n","\telse:\n","\t\treturn False\n","\n","def ch_partitioning(vertex, G):\n","\t\"\"\"\n","\tINPUT:\n","\t - \"vertex\" splitting vertex\n","\t - \"G\" Networkx Undirected Graph\n","\tOUTPUT:\n","\t - \"SSG\" Left subgraph after partitioning\n","\t - \"SG\" Right subgraph after partitioning\n","\t\"\"\"\n","\tn = list(G.neighbors(vertex))\n","\tGp = []\n","\tfor iter in list(G.edges()):\n","\t\tif iter[0] in n:\n","\t\t\tif iter[1] in n:\n","\t\t\t\tGp.append(iter)\n","\tG.remove_node(vertex)\n","\treturn nx.Graph(Gp), G\n","\n","def lowest_degree_vertex(graph):\n","\t\"\"\"\n","\tINPUT:\n","\t - \"graph\" Networkx Undirected Graph\n","\tOUTPUT:\n","\t - \"i\" node that has the lowest degree in the graph\n","\t\"\"\"\n","\tdegrees = [graph.degree(a) for a in list(graph.nodes())]\n","\tminimum = min(degrees)\n","\tfor i in list(graph.nodes()):\n","\t\tif graph.degree(i) == minimum:\n","\t\t\treturn i\n","\n","def highest_degree_vertex(graph):\n","\t\"\"\"\n","\tINPUT:\n","\t - \"graph\" Networkx Undirected Graph\n","\tOUTPUT:\n","\t - \"i\" node that has the lowest degree in the graph\n","\t\"\"\"\n","\tdegrees = [graph.degree(a) for a in list(graph.nodes())]\n","\tmaximum = max(degrees)\n","\tfor i in list(graph.nodes()):\n","\t\tif graph.degree(i) == maximum:\n","\t\t\treturn i\n","\n","def remove_zero_degree_nodes(graph):\n","    \"\"\"\n","    INPUT:\n","    - \"graph\" Networkx Undirected Graph\n","    OUTPUT:\n","    - \"graph\" Networkx Undirected Graph with no zero degree nodes\n","    \"\"\"\n","    nodes = list(graph.nodes())\n","    for n in nodes:\n","      if graph.degree(n) == 0:\n","        graph.remove_node(n)\n","    return graph\n","\n","# average of |G| - |SG| and |G| - |SSG|\n","def average_node_decrease(original_graph_size, subgraph1_size, subgraph2_size, bound_improve):\n","  if subgraph1_size == 0 and subgraph2_size == 0:\n","    return original_graph_size\n","  elif subgraph1_size == 0:\n","    return original_graph_size - subgraph2_size\n","  elif subgraph2_size == 0:\n","    return original_graph_size - subgraph1_size\n","  else:\n","    return (2*original_graph_size - subgraph2_size - subgraph1_size)/2"],"metadata":{"id":"bL38BWepeBbp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import matplotlib.pyplot as plt\n","from torch_geometric.nn import GCNConv\n","import torch.nn.functional as F"],"metadata":{"id":"wwIbDxZAd2Ld"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def ch_partitioning_eval(G, LIMIT, eval_function):\n","  k = mc_lower_bound(G)\n","  og_bound = len(k)\n","  graph = k_core_reduction(G, len(k))\n","  og_size = len(G)\n","  eval_list = []\n","  node_eval_list = []\n","  best_eval = -10000\n","  best_node = 0\n","  for vertex in tqdm(list(G.nodes())):\n","    graph = G.copy()\n","    vertex_removals = {graph: []}\n","    SG = graph\n","    subgraphs = [graph]\n","    vcount = vertex_removals[SG]\n","    del vertex_removals[SG]\n","    SSG, SG = ch_partitioning(vertex, graph)\n","    SG = remove_zero_degree_nodes(SG)\n","    SSG = remove_zero_degree_nodes(SSG)\n","    SG = k_core_reduction(SG, len(k)-len(vcount)) #0\n","    SSG = k_core_reduction(SSG, len(k)-len(vcount+[vertex])) #1\n","    vertex_removals[SSG] = vcount+[vertex]\n","    vertex_removals[SG] = vcount\n","    #####################################################################################################\n","    if is_clique(G.subgraph(list(SSG.nodes()))) == True:\n","      assert is_clique(G.subgraph(list(SSG.nodes())+vertex_removals[SSG])) == True\n","      if len(SSG)+len(vertex_removals[SSG]) > len(k):\n","        k = list(SSG.nodes())+vertex_removals[SSG]\n","      del vertex_removals[SSG]\n","      SSG = nx.Graph()\n","    if is_clique(G.subgraph(list(SG.nodes()))) == True:\n","      assert is_clique(G.subgraph(list(SG.nodes())+vertex_removals[SG])) == True\n","      if len(SG)+len(vertex_removals[SG]) > len(k):\n","        k = list(SG.nodes())+vertex_removals[SG]\n","      del vertex_removals[SG]\n","      SG = nx.Graph()\n","    #####################################################################################################\n","    if len(SSG) != 0:\n","      SSG_lower = mc_lower_bound(SSG)+vertex_removals[SSG]\n","      #print(vertex_removals[SSG])\n","      assert is_clique(G.subgraph(SSG_lower)) == True\n","      if len(SSG_lower) > len(k):\n","        vcount = vertex_removals[SSG]\n","        del vertex_removals[SSG]\n","        k = SSG_lower\n","        SSG = k_core_reduction(SSG, len(k)-len(vcount))\n","        SSG = remove_zero_degree_nodes(SSG)\n","        vertex_removals[SSG] = vcount\n","      if len(SSG) != 0:\n","        SSG_upper = mc_upper_bound(SSG)+len(vertex_removals[SSG])\n","        if SSG_upper > len(k):\n","          if len(SSG) <= LIMIT:\n","            del vertex_removals[SSG]\n","          else:\n","            subgraphs.append(SSG)\n","        else:\n","          del vertex_removals[SSG]\n","    if len(SSG) == 0:\n","      if SSG in list(vertex_removals.keys()):\n","        sub_solution_SSG = vertex_removals[SSG]\n","        del vertex_removals[SSG]\n","        assert is_clique(G.subgraph(sub_solution_SSG)) == True\n","        if len(sub_solution_SSG) > len(k):\n","          k = sub_solution_SSG\n","    #####################################################################################################\n","    if len(SG) != 0:\n","      SG_lower = mc_lower_bound(SG)+vertex_removals[SG]\n","      #print(vertex_removals[SG])\n","      assert is_clique(G.subgraph(SG_lower)) == True\n","      if len(SG_lower) > len(k):\n","        vcount = vertex_removals[SG]\n","        del vertex_removals[SG]\n","        k = SG_lower\n","        SG = k_core_reduction(SG, len(k)-len(vcount))\n","        SG = remove_zero_degree_nodes(SG)\n","        vertex_removals[SG] = vcount\n","      if len(SG) != 0:\n","        SG_upper = mc_upper_bound(SG)+len(vertex_removals[SG])\n","        if SG_upper > len(k):\n","          if len(SG) <= LIMIT:\n","            del vertex_removals[SG]\n","          else:\n","            subgraphs.append(SG)\n","        else:\n","          del vertex_removals[SG]\n","    if len(SG) == 0:\n","      if SG in list(vertex_removals.keys()):\n","        sub_solution_SG = vertex_removals[SG]\n","        del vertex_removals[SG]\n","        assert is_clique(G.subgraph(sub_solution_SG)) == True\n","        if len(sub_solution_SG) > len(k):\n","          k = sub_solution_SG\n","    # define eval parameters\n","    SG_size =  len(SG)\n","    if SG_size <= LIMIT:\n","      SG_size = 0\n","    SSG_size = len(SSG)\n","    if SSG_size <= LIMIT:\n","      SSG_size = 0\n","    bound_improvement = len(k) - og_bound\n","    current_eval = eval_function(og_size, SG_size, SSG_size, bound_improvement)\n","    eval_list.append(current_eval)\n","    node_eval_list.append(vertex)\n","    if current_eval > best_eval:\n","      best_eval = current_eval\n","      best_node = vertex\n","  temp = [[x, y] for x, y in zip(node_eval_list, eval_list)]\n","  node_eval_list = temp\n","  return node_eval_list, best_eval, best_node"],"metadata":{"id":"I1rrE7DJlKwR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generating Labels Function\n","def label_generation(graph, LIMIT=40, eval_function=average_node_decrease):\n","  \"\"\"\n","  Input: networkx graph, decomposition LIMIT\n","  - Preprocess graph (kcore)\n","  - Assign labels to each node\n","  Output: networkx graph with node labels\n","  \"\"\"\n","  assert type(graph) is nx.Graph\n","  assert type(LIMIT) is int\n","  assert len(graph) != 0\n","  print(\"=== Starting Labelling Algorithm ===\")\n","  G = graph.copy()\n","  atom_sizes = []\n","  if len(graph) <= LIMIT:\n","    print(\"=== Input Graph Size is Smaller than LIMIT ===\")\n","    return\n","  print(\"Preprocessing...\")\n","  graph = remove_zero_degree_nodes(graph)\n","  k = mc_lower_bound(graph)\n","  vertex_removal = {graph: []}\n","  subgraphs = [graph]\n","  SG = subgraphs.pop()\n","  SG = remove_zero_degree_nodes(SG)\n","  assert len(SG) != 0\n","  vcount = vertex_removal[SG]\n","  del vertex_removal[SG]\n","  print(\"Computing Labels\")\n","  SG_copy = SG.copy()\n","  eval_list,_,_ = ch_partitioning_eval(SG_copy, LIMIT, eval_function)\n","  for node_eval in eval_list:\n","    node = node_eval[0]\n","    eval = node_eval[1]\n","    G.nodes[node]['label'] = eval\n","  return G\n","\n","def max_normalize(label_list, k):\n","  # normalize data\n","  max_val = max(label_list)\n","  normalized_data = [(x / max_val) for x in label_list]\n","  return normalized_data\n","\n","\n","# Normalize dataset function\n","def dataset_normalize(dataset, normalize_function=max_normalize, normalize_param=0.2):\n","  for graph_idx in tqdm(range(len(dataset))):\n","    data = dataset[graph_idx]\n","    node_labels_list = data.y.tolist()\n","    normalized_node_labels_list = normalize_function(node_labels_list, normalize_param)\n","    data.y = torch.tensor(normalized_node_labels_list)\n","  return dataset\n","\n"],"metadata":{"id":"OEioKsXVltxi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def DBK_model_instance(pygraph, model, LIMIT=40):\n","  \"\"\"\n","  INPUT:\n","    - \"graph\" must be a Networkx Undirected Graph\n","    - \"LIMIT\" is an integer describing the largest size of graph which solver_func can solve; all subgraph sizes solved will be less than or equal to LIMIT\n","    - \"solver_function\" takes a Networkx Graph, and outputs a list of nodes which are hopefully the Maximum Clique elements; it can be an approximate or exact solver function\n","  OUTPUT:\n","    - \"k\" is a list of graph nodes which form a clique in the input graph. If the solver is exact, then k is the Maximum Clique\n","  NOTES:\n","    - The central idea of using bounds is that we maintain a global lower bound on the Maximum Clique. Then, for each sub problem we calculate a fast upper bound.\n","      If any sub problem has an upper bound which is less than or equal to the global lower bound, we can remove that sub problem from consideration in the remaining iterations of the algorithm.\n","    - This algorithm does not necessarily enumerate all cliques nor all Maximum Cliques. In particular, it is designed to return a single maximum clique assuming the solver is exact.\n","      However, the algorithm could be modified to include all maximum cliques found from solving each sub-problem.\n","    - There are many assert statements in this function. These all serve as \"sanity checks\"; if any of them are tripped, something went wrong or an input was incorrect\n","  \"\"\"\n","  start_time = time.time()\n","  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","  model.to(device)\n","  print(\"Model's device:\", next(model.parameters()).device)\n","  graph = to_networkx(pygraph)\n","  graph.remove_edges_from(nx.selfloop_edges(graph))\n","  # TRAIN THE MODEL\n","  labeled_graph = label_generation(graph)\n","  labeled_pygraph = graph_to_pyg_data(labeled_graph)\n","  labeled_pygraph.x = get_features(labeled_graph)\n","  labeled_pygraph.x = labeled_pygraph.x.to(torch.float32)\n","  normalized_pygraph = dataset_normalize([labeled_pygraph])[0]\n","  criterion = nn.MSELoss()\n","  optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","  # Train the model with validation\n","  normalized_pygraph.to(device)\n","  train_model_regression(model, [normalized_pygraph], [normalized_pygraph], optimizer, criterion, batch_size=1, patience=100, epochs=2000)\n","  model.eval()\n","  assert type(graph) is nx.Graph\n","  assert type(LIMIT) is int\n","  assert len(graph) != 0\n","  print(\"=== Starting DBK Algorithm ===\")\n","  G = graph.copy()\n","  num_of_atoms = 0\n","  iteration = 0\n","  atom_sizes = []\n","  if len(graph) <= LIMIT:\n","    print(\"=== Input Graph Size is Smaller than LIMIT ===\")\n","    num_of_atoms +=1\n","    print(\"=== Finished DBK Algorithm ===\")\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","    return num_of_atoms, iteration, atom_sizes, elapsed_time\n","  print(\"Preprocessing...\")\n","  graph = remove_zero_degree_nodes(graph)\n","  k = mc_lower_bound(graph)\n","  graph = k_core_reduction(graph, len(k))\n","  if len(graph) == 0:\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","    return num_of_atoms, iteration, atom_sizes, elapsed_time\n","  if len(graph) <= LIMIT:\n","    print(\"=== After K-core Reduction the Graph Size is Smaller than LIMIT ===\")\n","    num_of_atoms += 1\n","    print(\"=== Finished DBK Algorithm ===\")\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","    return num_of_atoms, iteration, atom_sizes, elapsed_time\n","  vertex_removal = {graph: []}\n","  subgraphs = [graph]\n","  while len(subgraphs) != 0:\n","    iteration += 1\n","    #print(\"Iteration number:\", iteration)\n","    SG = subgraphs.pop()\n","    SG = remove_zero_degree_nodes(SG)\n","    #print(\"Current subgraph size:\", len(SG))\n","    assert len(SG) != 0\n","    vcount = vertex_removal[SG]\n","    del vertex_removal[SG]\n","    SG_copy = SG.copy()\n","    # CHANGE GRAPH TO PY DATA OBJECT AND GET FEATURES\n","    new_py = graph_to_pyg_data(SG_copy)\n","    node_names = torch.unique(new_py.edge_index)\n","    node_names_list = node_names.tolist()\n","    num_nodes = new_py.num_nodes\n","    new_node_names = torch.arange(0, num_nodes)\n","    node_mapping = dict(zip(node_names.numpy(), new_node_names.numpy()))\n","    new_py.edge_index = torch.tensor([[node_mapping[edge[0].item()], node_mapping[edge[1].item()]] for edge in new_py.edge_index.t()])\n","    new_py.edge_index = new_py.edge_index.t()\n","    new_py.x = get_features(SG_copy)\n","    new_py.x = new_py.x.to(torch.float32)\n","    new_py = new_py.to(device)\n","    vertex = model_vertex_selection(model, new_py)\n","    vertex = list(SG_copy.nodes)[vertex]\n","    # REST OF THE ALGO\n","    SSG, SG = ch_partitioning(vertex, SG)\n","    SG = remove_zero_degree_nodes(SG) # BIG\n","    SSG = remove_zero_degree_nodes(SSG) # SMALL\n","    SG = k_core_reduction(SG, len(k)-len(vcount)) # 0\n","    SSG = k_core_reduction(SSG, len(k)-len(vcount+[vertex])) # 1\n","    vertex_removal[SSG] = vcount+[vertex]\n","    vertex_removal[SG] = vcount\n","    #####################################################################################################\n","    if is_clique(G.subgraph(list(SSG.nodes()))) == True:\n","      assert is_clique(G.subgraph(list(SSG.nodes())+vertex_removal[SSG])) == True\n","      if len(SSG)+len(vertex_removal[SSG]) > len(k):\n","        k = list(SSG.nodes())+vertex_removal[SSG]\n","      del vertex_removal[SSG]\n","      SSG = nx.Graph()\n","    if is_clique(G.subgraph(list(SG.nodes()))) == True:\n","      assert is_clique(G.subgraph(list(SG.nodes())+vertex_removal[SG])) == True\n","      if len(SG)+len(vertex_removal[SG]) > len(k):\n","        k = list(SG.nodes())+vertex_removal[SG]\n","      del vertex_removal[SG]\n","      SG = nx.Graph()\n","    #####################################################################################################\n","    if len(SSG) != 0:\n","      SSG_lower = mc_lower_bound(SSG)+vertex_removal[SSG]\n","      assert is_clique(G.subgraph(SSG_lower)) == True\n","      if len(SSG_lower) > len(k):\n","        vcount = vertex_removal[SSG]\n","        del vertex_removal[SSG]\n","        k = SSG_lower\n","        SSG = k_core_reduction(SSG, len(k)-len(vcount))\n","        SSG = remove_zero_degree_nodes(SSG)\n","        vertex_removal[SSG] = vcount\n","      if len(SSG) != 0:\n","        SSG_upper = mc_upper_bound(SSG)+len(vertex_removal[SSG])\n","        if SSG_upper > len(k):\n","          if len(SSG) <= LIMIT:\n","            #print(\"=== Terminal Subgraph Found ===\")\n","            #print(\"Size:\", len(SSG))\n","            atom_sizes.append(len(SSG))\n","            num_of_atoms += 1\n","            del vertex_removal[SSG]\n","          else:\n","            subgraphs.append(SSG)\n","        else:\n","          del vertex_removal[SSG]\n","    if len(SSG) == 0:\n","      if SSG in list(vertex_removal.keys()):\n","        sub_solution_SSG = vertex_removal[SSG]\n","        del vertex_removal[SSG]\n","        assert is_clique(G.subgraph(sub_solution_SSG)) == True\n","        if len(sub_solution_SSG) > len(k):\n","          k = sub_solution_SSG\n","    #####################################################################################################\n","    if len(SG) != 0:\n","      SG_lower = mc_lower_bound(SG)+vertex_removal[SG]\n","      assert is_clique(G.subgraph(SG_lower)) == True\n","      if len(SG_lower) > len(k):\n","        vcount = vertex_removal[SG]\n","        del vertex_removal[SG]\n","        k = SG_lower\n","        SG = k_core_reduction(SG, len(k)-len(vcount))\n","        SG = remove_zero_degree_nodes(SG)\n","        vertex_removal[SG] = vcount\n","      if len(SG) != 0:\n","        SG_upper = mc_upper_bound(SG)+len(vertex_removal[SG])\n","        if SG_upper > len(k):\n","          if len(SG) <= LIMIT:\n","            #print(\"=== Terminal Subgraph Found ===\")\n","            #print(\"Size:\", len(SG))\n","            atom_sizes.append(len(SG))\n","            num_of_atoms += 1\n","            # sub_solution_SG = solver_function(SG)+vertex_removal[SG]\n","            del vertex_removal[SG]\n","          else:\n","            subgraphs.append(SG)\n","        else:\n","          del vertex_removal[SG]\n","    if len(SG) == 0:\n","      if SG in list(vertex_removal.keys()):\n","        sub_solution_SG = vertex_removal[SG]\n","        del vertex_removal[SG]\n","        assert is_clique(G.subgraph(sub_solution_SG)) == True\n","        if len(sub_solution_SG) > len(k):\n","          k = sub_solution_SG\n","  assert len(vertex_removal) == 0\n","  print(\"=== Finished DBK Algorithm ===\")\n","  end_time = time.time()\n","  elapsed_time = end_time - start_time\n","  return num_of_atoms, iteration, atom_sizes, elapsed_time"],"metadata":{"id":"ds7vcWQknIHH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","import torch\n","\n","def DBK(graph, LIMIT=40):\n","  \"\"\"\n","  INPUT:\n","    - \"graph\" must be a Networkx Undirected Graph\n","    - \"LIMIT\" is an integer describing the largest size of graph which solver_func can solve; all subgraph sizes solved will be less than or equal to LIMIT\n","    - \"solver_function\" takes a Networkx Graph, and outputs a list of nodes which are hopefully the Maximum Clique elements; it can be an approximate or exact solver function\n","  OUTPUT:\n","    - \"k\" is a list of graph nodes which form a clique in the input graph. If the solver is exact, then k is the Maximum Clique\n","  NOTES:\n","    - The central idea of using bounds is that we maintain a global lower bound on the Maximum Clique. Then, for each sub problem we calculate a fast upper bound.\n","      If any sub problem has an upper bound which is less than or equal to the global lower bound, we can remove that sub problem from consideration in the remaining iterations of the algorithm.\n","    - This algorithm does not necessarily enumerate all cliques nor all Maximum Cliques. In particular, it is designed to return a single maximum clique assuming the solver is exact.\n","      However, the algorithm could be modified to include all maximum cliques found from solving each sub-problem.\n","    - There are many assert statements in this function. These all serve as \"sanity checks\"; if any of them are tripped, something went wrong or an input was incorrect\n","  \"\"\"\n","  start_time = time.time()\n","  assert type(graph) is nx.Graph\n","  assert type(LIMIT) is int\n","  assert len(graph) != 0\n","  print(\"=== Starting DBK Algorithm ===\")\n","  G = graph.copy()\n","  num_of_atoms = 0\n","  iteration = 0\n","  atom_sizes = []\n","  if len(graph) <= LIMIT:\n","    print(\"=== Input Graph Size is Smaller than LIMIT ===\")\n","    num_of_atoms +=1\n","    print(\"=== Finished DBK Algorithm ===\")\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","    return num_of_atoms, iteration, atom_sizes, elapsed_time\n","  print(\"Preprocessing...\")\n","  graph = remove_zero_degree_nodes(graph)\n","  k = mc_lower_bound(graph)\n","  graph = k_core_reduction(graph, len(k))\n","  if len(graph) == 0:\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","    return num_of_atoms, iteration, atom_sizes, elapsed_time\n","  if len(graph) <= LIMIT:\n","    print(\"=== After K-core Reduction the Graph Size is Smaller than LIMIT ===\")\n","    num_of_atoms += 1\n","    print(\"=== Finished DBK Algorithm ===\")\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","    return num_of_atoms, iteration, atom_sizes, elapsed_time\n","  vertex_removal = {graph: []}\n","  subgraphs = [graph]\n","  while len(subgraphs) != 0:\n","    iteration += 1\n","    SG = subgraphs.pop()\n","    SG = remove_zero_degree_nodes(SG)\n","    #print(\"Current subgraph size:\", len(SG))\n","    assert len(SG) != 0\n","    vcount = vertex_removal[SG]\n","    del vertex_removal[SG]\n","    vertex = lowest_degree_vertex(SG)\n","    SSG, SG = ch_partitioning(vertex, SG)\n","    SG = remove_zero_degree_nodes(SG) # BIG\n","    SSG = remove_zero_degree_nodes(SSG) # SMALL\n","    SG = k_core_reduction(SG, len(k)-len(vcount)) # 0\n","    SSG = k_core_reduction(SSG, len(k)-len(vcount+[vertex])) # 1\n","    vertex_removal[SSG] = vcount+[vertex]\n","    vertex_removal[SG] = vcount\n","    #####################################################################################################\n","    if is_clique(G.subgraph(list(SSG.nodes()))) == True:\n","      assert is_clique(G.subgraph(list(SSG.nodes())+vertex_removal[SSG])) == True\n","      if len(SSG)+len(vertex_removal[SSG]) > len(k):\n","        k = list(SSG.nodes())+vertex_removal[SSG]\n","      del vertex_removal[SSG]\n","      SSG = nx.Graph()\n","    if is_clique(G.subgraph(list(SG.nodes()))) == True:\n","      assert is_clique(G.subgraph(list(SG.nodes())+vertex_removal[SG])) == True\n","      if len(SG)+len(vertex_removal[SG]) > len(k):\n","        k = list(SG.nodes())+vertex_removal[SG]\n","      del vertex_removal[SG]\n","      SG = nx.Graph()\n","    #####################################################################################################\n","    if len(SSG) != 0:\n","      SSG_lower = mc_lower_bound(SSG)+vertex_removal[SSG]\n","      assert is_clique(G.subgraph(SSG_lower)) == True\n","      #print(\"SSG lower\", len(SSG_lower))\n","      #print(\"lowerbound:\", len(k))\n","      if len(SSG_lower) > len(k):\n","        vcount = vertex_removal[SSG]\n","        del vertex_removal[SSG]\n","        k = SSG_lower\n","        SSG = k_core_reduction(SSG, len(k)-len(vcount))\n","        SSG = remove_zero_degree_nodes(SSG)\n","        vertex_removal[SSG] = vcount\n","      if len(SSG) != 0:\n","        SSG_upper = mc_upper_bound(SSG)+len(vertex_removal[SSG])\n","        if SSG_upper > len(k):\n","          if len(SSG) <= LIMIT:\n","            #print(\"=== Terminal Subgraph Found ===\")\n","            #print(\"Size:\", len(SSG))\n","            atom_sizes.append(len(SSG))\n","            num_of_atoms += 1\n","            del vertex_removal[SSG]\n","          else:\n","            subgraphs.append(SSG)\n","        else:\n","          del vertex_removal[SSG]\n","    if len(SSG) == 0:\n","      if SSG in list(vertex_removal.keys()):\n","        sub_solution_SSG = vertex_removal[SSG]\n","        del vertex_removal[SSG]\n","        assert is_clique(G.subgraph(sub_solution_SSG)) == True\n","        if len(sub_solution_SSG) > len(k):\n","          k = sub_solution_SSG\n","    #####################################################################################################\n","    if len(SG) != 0:\n","      SG_lower = mc_lower_bound(SG)+vertex_removal[SG]\n","      assert is_clique(G.subgraph(SG_lower)) == True\n","      #print(\"SG lower\", len(SG_lower))\n","      #print(\"lowerbound:\", len(k))\n","      if len(SG_lower) > len(k):\n","        vcount = vertex_removal[SG]\n","        del vertex_removal[SG]\n","        k = SG_lower\n","        SG = k_core_reduction(SG, len(k)-len(vcount))\n","        SG = remove_zero_degree_nodes(SG)\n","        vertex_removal[SG] = vcount\n","      if len(SG) != 0:\n","        SG_upper = mc_upper_bound(SG)+len(vertex_removal[SG])\n","        if SG_upper > len(k):\n","          if len(SG) <= LIMIT:\n","            #print(\"=== Terminal Subgraph Found ===\")\n","            #print(\"Size:\", len(SG))\n","            atom_sizes.append(len(SG))\n","            num_of_atoms += 1\n","            # sub_solution_SG = solver_function(SG)+vertex_removal[SG]\n","            del vertex_removal[SG]\n","          else:\n","            subgraphs.append(SG)\n","        else:\n","          del vertex_removal[SG]\n","    if len(SG) == 0:\n","      if SG in list(vertex_removal.keys()):\n","        sub_solution_SG = vertex_removal[SG]\n","        del vertex_removal[SG]\n","        assert is_clique(G.subgraph(sub_solution_SG)) == True\n","        if len(sub_solution_SG) > len(k):\n","          k = sub_solution_SG\n","  assert len(vertex_removal) == 0\n","  print(\"=== Finished DBK Algorithm ===\")\n","  end_time = time.time()\n","  elapsed_time = end_time - start_time\n","  return num_of_atoms, iteration, atom_sizes, elapsed_time\n","\n","\n","def DBK_max(graph, LIMIT=40):\n","  \"\"\"\n","  INPUT:\n","    - \"graph\" must be a Networkx Undirected Graph\n","    - \"LIMIT\" is an integer describing the largest size of graph which solver_func can solve; all subgraph sizes solved will be less than or equal to LIMIT\n","    - \"solver_function\" takes a Networkx Graph, and outputs a list of nodes which are hopefully the Maximum Clique elements; it can be an approximate or exact solver function\n","  OUTPUT:\n","    - \"k\" is a list of graph nodes which form a clique in the input graph. If the solver is exact, then k is the Maximum Clique\n","  NOTES:\n","    - The central idea of using bounds is that we maintain a global lower bound on the Maximum Clique. Then, for each sub problem we calculate a fast upper bound.\n","      If any sub problem has an upper bound which is less than or equal to the global lower bound, we can remove that sub problem from consideration in the remaining iterations of the algorithm.\n","    - This algorithm does not necessarily enumerate all cliques nor all Maximum Cliques. In particular, it is designed to return a single maximum clique assuming the solver is exact.\n","      However, the algorithm could be modified to include all maximum cliques found from solving each sub-problem.\n","    - There are many assert statements in this function. These all serve as \"sanity checks\"; if any of them are tripped, something went wrong or an input was incorrect\n","  \"\"\"\n","  start_time = time.time()\n","  assert type(graph) is nx.Graph\n","  assert type(LIMIT) is int\n","  assert len(graph) != 0\n","  print(\"=== Starting DBK Algorithm ===\")\n","  G = graph.copy()\n","  num_of_atoms = 0\n","  iteration = 0\n","  atom_sizes = []\n","  if len(graph) <= LIMIT:\n","    print(\"=== Input Graph Size is Smaller than LIMIT ===\")\n","    num_of_atoms +=1\n","    print(\"=== Finished DBK Algorithm ===\")\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","    return num_of_atoms, iteration, atom_sizes, elapsed_time\n","  print(\"Preprocessing...\")\n","  graph = remove_zero_degree_nodes(graph)\n","  k = mc_lower_bound(graph)\n","  graph = k_core_reduction(graph, len(k))\n","  if len(graph) == 0:\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","    return num_of_atoms, iteration, atom_sizes, elapsed_time\n","  if len(graph) <= LIMIT:\n","    print(\"=== After K-core Reduction the Graph Size is Smaller than LIMIT ===\")\n","    num_of_atoms += 1\n","    print(\"=== Finished DBK Algorithm ===\")\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","    return num_of_atoms, iteration, atom_sizes, elapsed_time\n","  vertex_removal = {graph: []}\n","  subgraphs = [graph]\n","  while len(subgraphs) != 0:\n","    iteration += 1\n","    SG = subgraphs.pop()\n","    SG = remove_zero_degree_nodes(SG)\n","    #print(\"Current subgraph size:\", len(SG))\n","    assert len(SG) != 0\n","    vcount = vertex_removal[SG]\n","    del vertex_removal[SG]\n","    vertex = highest_degree_vertex(SG)\n","    SSG, SG = ch_partitioning(vertex, SG)\n","    SG = remove_zero_degree_nodes(SG) # BIG\n","    SSG = remove_zero_degree_nodes(SSG) # SMALL\n","    SG = k_core_reduction(SG, len(k)-len(vcount)) # 0\n","    SSG = k_core_reduction(SSG, len(k)-len(vcount+[vertex])) # 1\n","    vertex_removal[SSG] = vcount+[vertex]\n","    vertex_removal[SG] = vcount\n","    #####################################################################################################\n","    if is_clique(G.subgraph(list(SSG.nodes()))) == True:\n","      assert is_clique(G.subgraph(list(SSG.nodes())+vertex_removal[SSG])) == True\n","      if len(SSG)+len(vertex_removal[SSG]) > len(k):\n","        k = list(SSG.nodes())+vertex_removal[SSG]\n","      del vertex_removal[SSG]\n","      SSG = nx.Graph()\n","    if is_clique(G.subgraph(list(SG.nodes()))) == True:\n","      assert is_clique(G.subgraph(list(SG.nodes())+vertex_removal[SG])) == True\n","      if len(SG)+len(vertex_removal[SG]) > len(k):\n","        k = list(SG.nodes())+vertex_removal[SG]\n","      del vertex_removal[SG]\n","      SG = nx.Graph()\n","    #####################################################################################################\n","    if len(SSG) != 0:\n","      SSG_lower = mc_lower_bound(SSG)+vertex_removal[SSG]\n","      assert is_clique(G.subgraph(SSG_lower)) == True\n","      #print(\"SSG lower\", len(SSG_lower))\n","      #print(\"lowerbound:\", len(k))\n","      if len(SSG_lower) > len(k):\n","        vcount = vertex_removal[SSG]\n","        del vertex_removal[SSG]\n","        k = SSG_lower\n","        SSG = k_core_reduction(SSG, len(k)-len(vcount))\n","        SSG = remove_zero_degree_nodes(SSG)\n","        vertex_removal[SSG] = vcount\n","      if len(SSG) != 0:\n","        SSG_upper = mc_upper_bound(SSG)+len(vertex_removal[SSG])\n","        if SSG_upper > len(k):\n","          if len(SSG) <= LIMIT:\n","            #print(\"=== Terminal Subgraph Found ===\")\n","            #print(\"Size:\", len(SSG))\n","            atom_sizes.append(len(SSG))\n","            num_of_atoms += 1\n","            del vertex_removal[SSG]\n","          else:\n","            subgraphs.append(SSG)\n","        else:\n","          del vertex_removal[SSG]\n","    if len(SSG) == 0:\n","      if SSG in list(vertex_removal.keys()):\n","        sub_solution_SSG = vertex_removal[SSG]\n","        del vertex_removal[SSG]\n","        assert is_clique(G.subgraph(sub_solution_SSG)) == True\n","        if len(sub_solution_SSG) > len(k):\n","          k = sub_solution_SSG\n","    #####################################################################################################\n","    if len(SG) != 0:\n","      SG_lower = mc_lower_bound(SG)+vertex_removal[SG]\n","      assert is_clique(G.subgraph(SG_lower)) == True\n","      #print(\"SG lower\", len(SG_lower))\n","      #print(\"lowerbound:\", len(k))\n","      if len(SG_lower) > len(k):\n","        vcount = vertex_removal[SG]\n","        del vertex_removal[SG]\n","        k = SG_lower\n","        SG = k_core_reduction(SG, len(k)-len(vcount))\n","        SG = remove_zero_degree_nodes(SG)\n","        vertex_removal[SG] = vcount\n","      if len(SG) != 0:\n","        SG_upper = mc_upper_bound(SG)+len(vertex_removal[SG])\n","        if SG_upper > len(k):\n","          if len(SG) <= LIMIT:\n","            #print(\"=== Terminal Subgraph Found ===\")\n","            #print(\"Size:\", len(SG))\n","            atom_sizes.append(len(SG))\n","            num_of_atoms += 1\n","            # sub_solution_SG = solver_function(SG)+vertex_removal[SG]\n","            del vertex_removal[SG]\n","          else:\n","            subgraphs.append(SG)\n","        else:\n","          del vertex_removal[SG]\n","    if len(SG) == 0:\n","      if SG in list(vertex_removal.keys()):\n","        sub_solution_SG = vertex_removal[SG]\n","        del vertex_removal[SG]\n","        assert is_clique(G.subgraph(sub_solution_SG)) == True\n","        if len(sub_solution_SG) > len(k):\n","          k = sub_solution_SG\n","  assert len(vertex_removal) == 0\n","  print(\"=== Finished DBK Algorithm ===\")\n","  end_time = time.time()\n","  elapsed_time = end_time - start_time\n","  return num_of_atoms, iteration, atom_sizes, elapsed_time\n","\n","def DBK_model(pygraph, rank_model, LIMIT=40):\n","  \"\"\"\n","  INPUT:\n","    - \"graph\" must be a Networkx Undirected Graph\n","    - \"LIMIT\" is an integer describing the largest size of graph which solver_func can solve; all subgraph sizes solved will be less than or equal to LIMIT\n","    - \"solver_function\" takes a Networkx Graph, and outputs a list of nodes which are hopefully the Maximum Clique elements; it can be an approximate or exact solver function\n","  OUTPUT:\n","    - \"k\" is a list of graph nodes which form a clique in the input graph. If the solver is exact, then k is the Maximum Clique\n","  NOTES:\n","    - The central idea of using bounds is that we maintain a global lower bound on the Maximum Clique. Then, for each sub problem we calculate a fast upper bound.\n","      If any sub problem has an upper bound which is less than or equal to the global lower bound, we can remove that sub problem from consideration in the remaining iterations of the algorithm.\n","    - This algorithm does not necessarily enumerate all cliques nor all Maximum Cliques. In particular, it is designed to return a single maximum clique assuming the solver is exact.\n","      However, the algorithm could be modified to include all maximum cliques found from solving each sub-problem.\n","    - There are many assert statements in this function. These all serve as \"sanity checks\"; if any of them are tripped, something went wrong or an input was incorrect\n","  \"\"\"\n","  start_time = time.time()\n","  features = pygraph.x\n","  print(\"all features:\", features.shape)\n","  graph = to_networkx(pygraph)\n","  assert type(graph) is nx.Graph\n","  assert type(LIMIT) is int\n","  assert len(graph) != 0\n","  print(\"=== Starting DBK Algorithm ===\")\n","  G = graph.copy()\n","  num_of_atoms = 0\n","  iteration = 0\n","  atom_sizes = []\n","  if len(graph) <= LIMIT:\n","    print(\"=== Input Graph Size is Smaller than LIMIT ===\")\n","    num_of_atoms +=1\n","    print(\"=== Finished DBK Algorithm ===\")\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","    return num_of_atoms, iteration, atom_sizes, elapsed_time\n","  print(\"Preprocessing...\")\n","  graph = remove_zero_degree_nodes(graph)\n","  k = mc_lower_bound(graph)\n","  graph = k_core_reduction(graph, len(k))\n","  if len(graph) == 0:\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","    return num_of_atoms, iteration, atom_sizes, elapsed_time\n","  if len(graph) <= LIMIT:\n","    print(\"=== After K-core Reduction the Graph Size is Smaller than LIMIT ===\")\n","    num_of_atoms += 1\n","    print(\"=== Finished DBK Algorithm ===\")\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","    return num_of_atoms, iteration, atom_sizes, elapsed_time\n","  vertex_removal = {graph: []}\n","  subgraphs = [graph]\n","  while len(subgraphs) != 0:\n","    iteration += 1\n","    #print(\"Iteration number:\", iteration)\n","    SG = subgraphs.pop()\n","    SG = remove_zero_degree_nodes(SG)\n","    #print(\"Current subgraph size:\", len(SG))\n","    assert len(SG) != 0\n","    vcount = vertex_removal[SG]\n","    del vertex_removal[SG]\n","    SG_copy = SG.copy()\n","    new_py = graph_to_pyg_data(SG_copy)\n","    # node names\n","    node_names = torch.unique(new_py.edge_index)\n","    node_names_list = node_names.tolist()\n","    num_nodes = new_py.num_nodes\n","    new_node_names = torch.arange(0, num_nodes)\n","    node_mapping = dict(zip(node_names.numpy(), new_node_names.numpy()))\n","    new_py.edge_index = torch.tensor([[node_mapping[edge[0].item()], node_mapping[edge[1].item()]] for edge in new_py.edge_index.t()])\n","    new_py.edge_index = new_py.edge_index.t()\n","    # node features\n","    new_features = features[node_names_list]\n","    new_py.x = new_features\n","    # Get the list of unique node indices (node names)\n","    vertex = model_vertex_selection(rank_model, new_py)\n","    vertex = list(SG_copy.nodes)[vertex]\n","    SSG, SG = ch_partitioning(vertex, SG)\n","    SG = remove_zero_degree_nodes(SG) # BIG\n","    SSG = remove_zero_degree_nodes(SSG) # SMALL\n","    SG = k_core_reduction(SG, len(k)-len(vcount)) # 0\n","    SSG = k_core_reduction(SSG, len(k)-len(vcount+[vertex])) # 1\n","    vertex_removal[SSG] = vcount+[vertex]\n","    vertex_removal[SG] = vcount\n","    #####################################################################################################\n","    if is_clique(G.subgraph(list(SSG.nodes()))) == True:\n","      assert is_clique(G.subgraph(list(SSG.nodes())+vertex_removal[SSG])) == True\n","      if len(SSG)+len(vertex_removal[SSG]) > len(k):\n","        k = list(SSG.nodes())+vertex_removal[SSG]\n","      del vertex_removal[SSG]\n","      SSG = nx.Graph()\n","    if is_clique(G.subgraph(list(SG.nodes()))) == True:\n","      assert is_clique(G.subgraph(list(SG.nodes())+vertex_removal[SG])) == True\n","      if len(SG)+len(vertex_removal[SG]) > len(k):\n","        k = list(SG.nodes())+vertex_removal[SG]\n","      del vertex_removal[SG]\n","      SG = nx.Graph()\n","    #####################################################################################################\n","    if len(SSG) != 0:\n","      SSG_lower = mc_lower_bound(SSG)+vertex_removal[SSG]\n","      assert is_clique(G.subgraph(SSG_lower)) == True\n","      if len(SSG_lower) > len(k):\n","        vcount = vertex_removal[SSG]\n","        del vertex_removal[SSG]\n","        k = SSG_lower\n","        SSG = k_core_reduction(SSG, len(k)-len(vcount))\n","        SSG = remove_zero_degree_nodes(SSG)\n","        vertex_removal[SSG] = vcount\n","      if len(SSG) != 0:\n","        SSG_upper = mc_upper_bound(SSG)+len(vertex_removal[SSG])\n","        if SSG_upper > len(k):\n","          if len(SSG) <= LIMIT:\n","            #print(\"=== Terminal Subgraph Found ===\")\n","            #print(\"Size:\", len(SSG))\n","            atom_sizes.append(len(SSG))\n","            num_of_atoms += 1\n","            del vertex_removal[SSG]\n","          else:\n","            subgraphs.append(SSG)\n","        else:\n","          del vertex_removal[SSG]\n","    if len(SSG) == 0:\n","      if SSG in list(vertex_removal.keys()):\n","        sub_solution_SSG = vertex_removal[SSG]\n","        del vertex_removal[SSG]\n","        assert is_clique(G.subgraph(sub_solution_SSG)) == True\n","        if len(sub_solution_SSG) > len(k):\n","          k = sub_solution_SSG\n","    #####################################################################################################\n","    if len(SG) != 0:\n","      SG_lower = mc_lower_bound(SG)+vertex_removal[SG]\n","      assert is_clique(G.subgraph(SG_lower)) == True\n","      if len(SG_lower) > len(k):\n","        vcount = vertex_removal[SG]\n","        del vertex_removal[SG]\n","        k = SG_lower\n","        SG = k_core_reduction(SG, len(k)-len(vcount))\n","        SG = remove_zero_degree_nodes(SG)\n","        vertex_removal[SG] = vcount\n","      if len(SG) != 0:\n","        SG_upper = mc_upper_bound(SG)+len(vertex_removal[SG])\n","        if SG_upper > len(k):\n","          if len(SG) <= LIMIT:\n","            #print(\"=== Terminal Subgraph Found ===\")\n","            #print(\"Size:\", len(SG))\n","            atom_sizes.append(len(SG))\n","            num_of_atoms += 1\n","            # sub_solution_SG = solver_function(SG)+vertex_removal[SG]\n","            del vertex_removal[SG]\n","          else:\n","            subgraphs.append(SG)\n","        else:\n","          del vertex_removal[SG]\n","    if len(SG) == 0:\n","      if SG in list(vertex_removal.keys()):\n","        sub_solution_SG = vertex_removal[SG]\n","        del vertex_removal[SG]\n","        assert is_clique(G.subgraph(sub_solution_SG)) == True\n","        if len(sub_solution_SG) > len(k):\n","          k = sub_solution_SG\n","  assert len(vertex_removal) == 0\n","  print(\"=== Finished DBK Algorithm ===\")\n","  end_time = time.time()\n","  elapsed_time = end_time - start_time\n","  return num_of_atoms, iteration, atom_sizes, elapsed_time\n"],"metadata":{"id":"27_iqEvffWly"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# GCN Layer\n","\n","class GCNLayer(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(GCNLayer, self).__init__()\n","        self.gcn = GCNConv(input_size, hidden_size)\n","\n","    def forward(self, x, edge_index):\n","        return self.gcn(x, edge_index)\n","\n","\n","class RegressionModel(nn.Module):\n","    def __init__(self, input_size, gcn_hidden_size, dropout_prob=0):\n","        super(RegressionModel, self).__init__()\n","        # GCN layers\n","        self.gcn1 = GCNLayer(input_size, gcn_hidden_size)\n","        self.gcn2 = GCNLayer(gcn_hidden_size, gcn_hidden_size)\n","        self.gcn3 = GCNLayer(gcn_hidden_size, gcn_hidden_size)\n","\n","        self.fc1 = nn.Linear(gcn_hidden_size, 128)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 32)\n","        self.fc4 = nn.Linear(32, 1)\n","        self.sigmoid = nn.Sigmoid()\n","        self.dropout = nn.Dropout(p=dropout_prob)\n","        self.batch_norm1 = nn.BatchNorm1d(128)\n","        self.batch_norm2 = nn.BatchNorm1d(64)\n","        self.batch_norm3 = nn.BatchNorm1d(32)\n","\n","    def forward(self, data):\n","        x, edge_index = data.x, data.edge_index\n","        x = self.gcn1(x, edge_index)\n","        x = self.relu(x)\n","        x = self.gcn2(x, edge_index)\n","        x = self.relu(x)\n","\n","        # MLP layers\n","        x = self.batch_norm1(self.fc1(x))\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        x = self.batch_norm2(self.fc2(x))\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        x = self.batch_norm3(self.fc3(x))\n","        x = self.relu(x)\n","        x = self.fc4(x)\n","        x = self.sigmoid(x)\n","        return x\n","\n","\n","def train_model_regression(model, train_loader, val_loader, optimizer, criterion, batch_size, patience, epochs=10):\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model.to(device)\n","    train_losses = []\n","    val_losses = []\n","    base_path = '/content/drive/MyDrive/Colab Notebooks/Decomp/Models/regression'\n","    best_train_loss = float('inf')\n","    consecutive_no_improvement = 0\n","    for epoch in range(epochs):\n","        model.train()\n","        running_loss = 0.0\n","        start = 0\n","        train_epoch_loss = 0.0\n","        total_length = len(train_loader) // batch_size\n","\n","\n","        for i in range(start, len(train_loader), batch_size):\n","            optimizer.zero_grad()\n","            scores = torch.empty(0, dtype=torch.float32, requires_grad=True).to(device)\n","            labels = torch.empty(0, dtype=torch.float32, requires_grad=True).to(device)\n","            for j in range(start, start + batch_size):\n","                if j >= len(train_loader):\n","                    break\n","                graph = train_loader[j]\n","                temp = model(graph)\n","                scores = torch.cat([scores, temp], dim=0)\n","                temp = graph.y\n","                temp = temp.to(device)\n","                labels = torch.cat([labels, temp], dim=0)\n","            start += batch_size\n","            scores = scores.squeeze().to(device)\n","            loss = criterion(scores, labels)\n","            train_epoch_loss += loss.item()\n","\n","\n","            # Backward pass and optimization\n","            loss.backward()\n","            optimizer.step()\n","\n","        average_train_loss = train_epoch_loss / total_length\n","        train_losses.append(average_train_loss)\n","\n","\n","        # Print progress\n","        if (epoch + 1) % 100 == 0:\n","            print(f'Epoch [{epoch+1}/{epochs}], '\n","                  f'Train Loss: {average_train_loss:.4f}')\n","        if train_losses:\n","            if min(train_losses) < best_train_loss:\n","                best_train_loss = min(train_losses)\n","                consecutive_no_improvement = 0\n","            else:\n","                consecutive_no_improvement += 1\n","            if consecutive_no_improvement >= patience:\n","                print(f'Early stopping at epoch {epoch+1} due to no improvement in training loss.')\n","                break\n","    # Plot the training and validation loss curves\n","    plt.plot(range(1, epoch+2), train_losses, label='Training Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.show()"],"metadata":{"id":"ubEsIcthmlCl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Defining some data preprocessing functions\n","def graph_to_pyg_data(graph):\n","    edge_index = torch.tensor(list(graph.edges)).t().contiguous()\n","    x = torch.eye(graph.number_of_nodes())  # Node features, identity matrix in this example\n","    y = torch.tensor(list(nx.get_node_attributes(graph, 'label').values()))  # Node labels\n","\n","    return Data(x=x, edge_index=edge_index, y=y)\n","\n","def to_networkx(data):\n","    edge_index = data.edge_index.cpu().numpy()\n","    edge_attr = None\n","    if data.edge_attr is not None:\n","        edge_attr = data.edge_attr.cpu().numpy()\n","\n","    G = nx.Graph()\n","    G.add_nodes_from(range(data.num_nodes))\n","    G.add_edges_from(edge_index.T)\n","\n","    if edge_attr is not None:\n","        for i, (src, tgt) in enumerate(edge_index.T):\n","            G[src][tgt]['edge_attr'] = edge_attr[i]\n","\n","    return G\n","\n","    # min max normalize [0, 1]\n","def min_max_normalize(data, new_min=0, new_max=1):\n","    # Find the min and max values in the data\n","    min_val = min(data)\n","    max_val = max(data)\n","    if min_val == max_val:\n","      return [1 for _ in range(len(data))]\n","    normalized_data = [(x - min_val) / (max_val - min_val) * (new_max - new_min) + new_min for x in data]\n","    return normalized_data\n","\n","# top k% of values are 1 rest are 0\n","def top_k(label_list, k):\n","    k = round(k*len(label_list))\n","    input_list = min_max_normalize(label_list)\n","    # Sort the list in descending order\n","    sorted_list = sorted(input_list, reverse=True)\n","    # Determine the threshold index\n","    threshold_index = min(k, len(sorted_list))\n","    # Set the first k elements to 1 and the rest to 0\n","    thresholded_list = [1 if i < threshold_index else 0 for i in range(len(sorted_list))]\n","    # Create a mapping from original indices to sorted indices\n","    index_mapping = {original: sorted_index for sorted_index, original in enumerate(sorted(range(len(input_list)), key=lambda x: input_list[x], reverse=True))}\n","    # Sort the thresholded list back to the original order\n","    thresholded_list_original_order = [thresholded_list[index_mapping[i]] for i in range(len(input_list))]\n","\n","    return thresholded_list_original_order\n","\n","# continuous [0,1]\n","def continuous(label_list, k):\n","  label_list = min_max_normalize(label_list)\n","  return label_list\n","\n","# value above k are 1 rest are 0\n","def within_k(label_list, k):\n","  k = 1-k\n","  label_list = min_max_normalize(label_list)\n","  data = [1 if x > k else 0 for x in label_list]\n","  return data\n","\n","\n","def max_normalize_binary(label_list, k):\n","  k = 1-k\n","  # normalize data\n","  max_val = max(label_list)\n","  normalized_data = [(x / max_val) for x in label_list]\n","  data = [1 if x > k else 0 for x in normalized_data]\n","  return data\n","\n","def max_normalize(label_list, k):\n","  # normalize data\n","  max_val = max(label_list)\n","  normalized_data = [(x / max_val) for x in label_list]\n","  return normalized_data\n","\n","\n","# TAKES IN A NETWORKX GRAPH AND OUTPUTS A TENSOR THAT IS THE NODE FEATURES\n","def get_features(nxgraph):\n","  # get features\n","  node_degrees = dict(nxgraph.degree())\n","  degree_centrality = nx.degree_centrality(nxgraph)\n","  betweenness_centrality = nx.betweenness_centrality(nxgraph)\n","  closeness_centrality = nx.closeness_centrality(nxgraph)\n","  eigenvector_centrality = nx.eigenvector_centrality(nxgraph, max_iter=500, tol=1.0e-3)\n","  pagerank_centrality = nx.pagerank(nxgraph, max_iter=500, tol=1.0e-3)\n","  harmonic_centrality = nx.harmonic_centrality(nxgraph)\n","  load_centrality = nx.load_centrality(nxgraph)\n","  clustering_coefficient = nx.clustering(nxgraph)\n","  # make it into an array\n","  features_array = np.array([\n","    list(node_degrees.values()),\n","    list(degree_centrality.values()),\n","    list(betweenness_centrality.values()),\n","    list(closeness_centrality.values()),\n","    list(eigenvector_centrality.values()),\n","    list(pagerank_centrality.values()),\n","    list(harmonic_centrality.values()),\n","    list(load_centrality.values()),\n","    list(clustering_coefficient.values())])\n","  features_array = features_array.T\n","  return torch.tensor(features_array)\n"],"metadata":{"id":"Lo9GGrZ8N0Wa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def model_vertex_selection(model, pygraph):\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model.to(device)\n","    model.eval()\n","    best_node = None\n","    best_value = -9999\n","    # get labels\n","    outputs = model(pygraph)\n","    temp = outputs.detach().cpu()\n","    outputs = temp.numpy()\n","    best_value = -9999\n","    best_node = None\n","    for node, output_value in enumerate(outputs):\n","        if output_value > best_value:\n","            best_value = output_value\n","            best_node = node\n","    return best_node"],"metadata":{"id":"gbeCLDQUe_aL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","loaded_dataset = torch.load('/content/NYSEgraph.pt')"],"metadata":{"id":"MPy3j0F3SMBa","colab":{"base_uri":"https://localhost:8080/","height":346},"executionInfo":{"status":"error","timestamp":1702870037667,"user_tz":480,"elapsed":123,"user":{"displayName":"Jia He Sun","userId":"07783893916712904513"}},"outputId":"44e3e20f-f59a-4f5b-e89b-71866dc4bf52"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-eb4f8408bb74>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloaded_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/NYSEgraph.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/NYSEgraph.pt'"]}]},{"cell_type":"code","source":["import numpy as np\n","from tqdm.auto import tqdm\n","\n","\n","num_of_atoms_dbk = []\n","iteration_dbk = []\n","time_dbk = []\n","\n","num_of_atoms_reg = []\n","iteration_reg = []\n","time_reg = []\n","\n","num_of_atoms_max = []\n","iteration_max = []\n","time_max = []\n","\n","\n","for i in tqdm(range(len(loaded_dataset))):\n","    print(i+1)\n","    pygraph = loaded_dataset\n","    nxgraph = to_networkx(pygraph)\n","    nxgraph.remove_edges_from(nx.selfloop_edges(nxgraph))\n","    reg_model = RegressionModel(9, 256)\n","    num_of_atoms1, iteration1, atom_sizes1, time1 = DBK_model_instance(pygraph, reg_model, LIMIT=40)\n","    num_of_atoms_reg.append(num_of_atoms1)\n","    iteration_reg.append(iteration1)\n","    time_reg.append(time1)\n","    np.save('/content/NYSE_num_of_atoms_reg.npy', num_of_atoms_reg)\n","    np.save('/content/NYSE_iteration_reg.npy', iteration_reg)\n","    np.save('/content/NYSE_time_reg.npy', time_reg)\n","    #num_of_atoms2, iteration2, atom_sizes2, time2 = DBK(nxgraph, LIMIT=40)\n","    #num_of_atoms_dbk.append(num_of_atoms2)\n","    #iteration_dbk.append(iteration2)\n","    #time_dbk.append(time2)\n","    #num_of_atoms3, iteration3, atom_sizes3, time3 = DBK_max(nxgraph, LIMIT=40)\n","    #num_of_atoms_max.append(num_of_atoms3)\n","    #iteration_max.append(iteration3)\n","    #time_max.append(time3)\n","\n","#np.save('/content/200_num_of_atoms_reg.npy', num_of_atoms_reg)\n","#np.save('/content/200_iteration_reg.npy', iteration_reg)\n","#np.save('/content/200_time_reg.npy', time_reg)\n","#np.save('/content/200_num_of_atoms_dbk.npy', num_of_atoms_dbk)\n","#np.save('/content/200_iteration_dbk.npy', iteration_dbk)\n","#np.save('/content/200_time_dbk.npy', time_dbk)\n","#np.save('/content/80_num_of_atoms_max.npy', num_of_atoms_max)\n","#np.save('/content/80_iteration_max.npy', iteration_max)\n","#np.save('/content/80_time_max.npy', time_max)"],"metadata":{"id":"gH9x_Z57r9iK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loaded_dataset = torch.load('/content/drive/MyDrive/Colab Notebooks/Decomp/LabeledData/NetworkxGraphs/test_200nodes_100graphs.pt')"],"metadata":{"id":"Dql1pCKQnFwV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["densities = []\n","for graph in loaded_dataset:\n","    nxgraph = to_networkx(graph)\n","    nxgraph.remove_edges_from(nx.selfloop_edges(nxgraph))\n","    densities.append(nx.density(nxgraph))\n","print(densities)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0R-ATIb--Ita","executionInfo":{"status":"ok","timestamp":1702870230260,"user_tz":480,"elapsed":2333,"user":{"displayName":"Jia He Sun","userId":"07783893916712904513"}},"outputId":"bb1bdb9a-80f5-47ec-876c-e0bc7bbf1cd6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.5048743718592965, 0.372964824120603, 0.6663316582914572, 0.20241206030150755, 0.5751256281407036, 0.44763819095477386, 0.14909547738693468, 0.3684924623115578, 0.20065326633165828, 0.5874874371859297, 0.41361809045226133, 0.5064321608040201, 0.16015075376884422, 0.125678391959799, 0.4948743718592965, 0.351608040201005, 0.4189949748743719, 0.33175879396984925, 0.6508542713567839, 0.3478894472361809, 0.4790954773869347, 0.2845226130653266, 0.5430653266331659, 0.4809547738693467, 0.33824120603015073, 0.4037185929648241, 0.624321608040201, 0.555929648241206, 0.4275376884422111, 0.345678391959799, 0.375427135678392, 0.30974874371859296, 0.5524623115577889, 0.21477386934673368, 0.30110552763819093, 0.21623115577889448, 0.2626633165829146, 0.6370854271356784, 0.3923115577889447, 0.4801005025125628, 0.37331658291457287, 0.20507537688442212, 0.6548743718592965, 0.13904522613065326, 0.3366331658291457, 0.6042211055276382, 0.1628643216080402, 0.5209045226130653, 0.5244221105527638, 0.13266331658291458, 0.37829145728643215, 0.6260804020100502, 0.6840703517587939, 0.2690452261306533, 0.46457286432160805, 0.6834673366834171, 0.6366834170854271, 0.2657788944723618, 0.4645226130653266, 0.6389949748743718, 0.6009547738693467, 0.22195979899497487, 0.33050251256281404, 0.5963316582914573, 0.6510552763819095, 0.5592462311557789, 0.4119095477386935, 0.22859296482412061, 0.22, 0.2436683417085427, 0.5250753768844221, 0.672713567839196, 0.3522613065326633, 0.275678391959799, 0.5826633165829146, 0.6061306532663316, 0.20683417085427136, 0.6202010050251257, 0.13753768844221106, 0.4605527638190955, 0.2565829145728643, 0.3907537688442211, 0.4966834170854271, 0.4707035175879397, 0.16160804020100503, 0.5014572864321608, 0.5610050251256281, 0.37035175879396987, 0.19095477386934673, 0.3803517587939699, 0.1885929648241206, 0.3168844221105528, 0.6728643216080402, 0.6039698492462312, 0.4623618090452261, 0.5310050251256282, 0.5833668341708542, 0.6610050251256281, 0.5169346733668342, 0.11090452261306533]\n"]}]},{"cell_type":"code","source":["np.save('/content/drive/MyDrive/Colab Notebooks/Decomp/Results/densities.npy', np.array(densities))"],"metadata":{"id":"HomRB1pj-OuF","executionInfo":{"status":"ok","timestamp":1702884654752,"user_tz":480,"elapsed":135,"user":{"displayName":"Jia He Sun","userId":"07783893916712904513"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SVPfcfjS1I9t"},"execution_count":null,"outputs":[]}]}