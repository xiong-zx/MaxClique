# -*- coding: utf-8 -*-
"""Evaluation

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fdVMptOX82aRJOXDS7jOPugsgzCKeO16
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install torch torchvision torchaudio
!pip install torch-geometric

import networkx as nx
import matplotlib.pyplot as plt

"""# HELPER FUNCTIONS"""

import networkx as nx
import random
import numpy


def mc_upper_bound(G):
	"""
	INPUT:
	 - "G" Networkx Undirected Graph
	OUTPUT:
	 - "chromatic_number" integer upper bound on the maximum clique number
	"""
	answ = nx.algorithms.coloring.greedy_color(G)
	chromatic_number = list(set(list(answ.values())))
	return len(chromatic_number)

def mc_lower_bound(G):
	"""
	INPUT:
	 - "G" Networkx Undirected Graph
	OUTPUT:
	 - "lower bound" list of variables which form a clique in G
	"""
	return nx.maximal_independent_set(nx.complement(G))

def edge_k_core(G, k):
	"""
	INPUT:
	 - "G" Networkx Undirected Graph
	 - "k" Integer that is at least one less than the global maximum clique number
	OUTPUT:
	 - "G" Networkx Undirected Graph where edge k-core reduction has been applied
	"""
	for a in list(G.edges()):
		x = list(G.neighbors(a[0]))
		y = list(G.neighbors(a[1]))
		if len(list(set(x) & set(y))) <= (k-2):
			G.remove_edge(a[0], a[1])
	return G

def k_core_reduction(graph, k):
	"""
	INPUT:
	 - "graph" Networkx Undirected Graph
	 - "k" Integer that is at least one less than the global maximum clique number
	OUTPUT:
	 - "graph" Networkx Undirected Graph where k-core reduction has been applied
	"""
	graph = nx.k_core(graph, k)
	ref1 = len(list(graph.edges()))
	graph = edge_k_core(graph, k)
	ref2 = len(list(graph.edges()))
	while ref1 != ref2:
		if len(graph) == 0:
			return graph
		graph = nx.k_core(graph, k)
		ref1 = len(list(graph.edges()))
		graph = edge_k_core(graph, k)
		ref2 = len(list(graph.edges()))
	return graph

def is_clique(G):
	"""
	INPUT:
	 - "G" Networkx Undirected Graph
	OUTPUT:
	 - "True" if G is a clique, and "False" if G is not a clique
	"""
	n = len(list(G.nodes()))
	m = len(list(G.edges()))
	if int(m) == int((n*(n-1))/float(2)):
		return True
	else:
		return False

def ch_partitioning(vertex, G):
	"""
	INPUT:
	 - "vertex" splitting vertex
	 - "G" Networkx Undirected Graph
	OUTPUT:
	 - "SSG" Left subgraph after partitioning
	 - "SG" Right subgraph after partitioning
	"""
	n = list(G.neighbors(vertex))
	Gp = []
	for iter in list(G.edges()):
		if iter[0] in n:
			if iter[1] in n:
				Gp.append(iter)
	G.remove_node(vertex)
	return nx.Graph(Gp), G

def lowest_degree_vertex(graph):
	"""
	INPUT:
	 - "graph" Networkx Undirected Graph
	OUTPUT:
	 - "i" node that has the lowest degree in the graph
	"""
	degrees = [graph.degree(a) for a in list(graph.nodes())]
	minimum = min(degrees)
	for i in list(graph.nodes()):
		if graph.degree(i) == minimum:
			return i

def remove_zero_degree_nodes(graph):
    """
    INPUT:
    - "graph" Networkx Undirected Graph
    OUTPUT:
    - "graph" Networkx Undirected Graph with no zero degree nodes
    """
    nodes = list(graph.nodes())
    for n in nodes:
      if graph.degree(n) == 0:
        graph.remove_node(n)
    return graph

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
from torch_geometric.nn import GCNConv
import torch.nn.functional as F

class GCNLayer(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(GCNLayer, self).__init__()
        self.gcn = GCNConv(input_size, hidden_size)

    def forward(self, x, edge_index):
        return self.gcn(x, edge_index)

class PairwiseRankingDataset(Dataset):
    def __init__(self, features, labels):
        self.features = features
        self.labels = labels

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

class BinaryClassificationModel(nn.Module):
    def __init__(self, input_size, gcn_hidden_size, dropout_prob=0.3):
        super(BinaryClassificationModel, self).__init__()
        # GCN layers
        self.gcn1 = GCNLayer(input_size, gcn_hidden_size)
        self.gcn2 = GCNLayer(gcn_hidden_size, gcn_hidden_size)

        self.fc1 = nn.Linear(gcn_hidden_size, 64)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 1)
        self.sigmoid = nn.Sigmoid()
        self.dropout = nn.Dropout(p=dropout_prob)
        self.batch_norm1 = nn.BatchNorm1d(64)
        self.batch_norm2 = nn.BatchNorm1d(32)
        self.batch_norm3 = nn.BatchNorm1d(1)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        # GCN layers
        x = self.gcn1(x, edge_index)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.gcn2(x, edge_index)
        x = self.relu(x)
        x = self.dropout(x)

        # MLP layers
        x = self.batch_norm1(self.fc1(x))
        x = self.relu(x)
        x = self.dropout(x)
        x = self.batch_norm2(self.fc2(x))
        x = self.relu(x)
        x = self.dropout(x)
        x = self.batch_norm3(self.fc3(x))
        # x = self.sigmoid(x)
        return x


class GCNLayer(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(GCNLayer, self).__init__()
        self.gcn = GCNConv(input_size, hidden_size)

    def forward(self, x, edge_index):
        return self.gcn(x, edge_index)

class PairwiseRankingDataset(Dataset):
    def __init__(self, features, labels):
        self.features = features
        self.labels = labels

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

class SimpleRankModel(nn.Module):
    def __init__(self, input_size, gcn_hidden_size, dropout_prob=0.3):
        super(SimpleRankModel, self).__init__()
        # GCN layers
        self.gcn1 = GCNLayer(input_size, gcn_hidden_size)
        self.gcn2 = GCNLayer(gcn_hidden_size, gcn_hidden_size)

        self.fc1 = nn.Linear(gcn_hidden_size, 64)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 1)
        self.sigmoid = nn.Sigmoid()
        self.dropout = nn.Dropout(p=dropout_prob)
        self.batch_norm1 = nn.BatchNorm1d(64)
        self.batch_norm2 = nn.BatchNorm1d(32)
        self.batch_norm3 = nn.BatchNorm1d(1)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        # GCN layers
        x = self.gcn1(x, edge_index)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.gcn2(x, edge_index)
        x = self.relu(x)
        x = self.dropout(x)

        # MLP layers
        x = self.batch_norm1(self.fc1(x))
        x = self.relu(x)
        x = self.dropout(x)
        x = self.batch_norm2(self.fc2(x))
        x = self.relu(x)
        x = self.dropout(x)
        x = self.batch_norm3(self.fc3(x))
        # x = self.sigmoid(x)
        return x

class GCNLayer(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(GCNLayer, self).__init__()
        self.gcn = GCNConv(input_size, hidden_size)

    def forward(self, x, edge_index):
        return self.gcn(x, edge_index)


class PairwiseRankingDataset(Dataset):
    def __init__(self, features, labels):
        self.features = features
        self.labels = labels

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

class GCNRankNet(nn.Module):
    def __init__(self, input_size, gcn_hidden_size, dropout_prob=0.3):
        super(GCNRankNet, self).__init__()
        # GCN layers
        self.gcn1 = GCNLayer(input_size, gcn_hidden_size)
        self.gcn2 = GCNLayer(gcn_hidden_size, gcn_hidden_size)

        self.fc1 = nn.Linear(gcn_hidden_size, 64)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 1)
        self.sigmoid = nn.Sigmoid()
        self.dropout = nn.Dropout(p=dropout_prob)
        self.batch_norm1 = nn.BatchNorm1d(64)
        self.batch_norm2 = nn.BatchNorm1d(32)
        self.batch_norm3 = nn.BatchNorm1d(1)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        # GCN layers
        x = self.gcn1(x, edge_index)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.gcn2(x, edge_index)
        x = self.relu(x)
        x = self.dropout(x)

        # MLP layers
        x = self.batch_norm1(self.fc1(x))
        x = self.relu(x)
        x = self.dropout(x)
        x = self.batch_norm2(self.fc2(x))
        x = self.relu(x)
        x = self.dropout(x)
        x = self.batch_norm3(self.fc3(x))
        # x = self.sigmoid(x)
        return x

import time

def DBK(graph, LIMIT=40):
  """
  INPUT:
    - "graph" must be a Networkx Undirected Graph
    - "LIMIT" is an integer describing the largest size of graph which solver_func can solve; all subgraph sizes solved will be less than or equal to LIMIT
    - "solver_function" takes a Networkx Graph, and outputs a list of nodes which are hopefully the Maximum Clique elements; it can be an approximate or exact solver function
  OUTPUT:
    - "k" is a list of graph nodes which form a clique in the input graph. If the solver is exact, then k is the Maximum Clique
  NOTES:
    - The central idea of using bounds is that we maintain a global lower bound on the Maximum Clique. Then, for each sub problem we calculate a fast upper bound.
      If any sub problem has an upper bound which is less than or equal to the global lower bound, we can remove that sub problem from consideration in the remaining iterations of the algorithm.
    - This algorithm does not necessarily enumerate all cliques nor all Maximum Cliques. In particular, it is designed to return a single maximum clique assuming the solver is exact.
      However, the algorithm could be modified to include all maximum cliques found from solving each sub-problem.
    - There are many assert statements in this function. These all serve as "sanity checks"; if any of them are tripped, something went wrong or an input was incorrect
  """
  start_time = time.time()
  assert type(graph) is nx.Graph
  assert type(LIMIT) is int
  assert len(graph) != 0
  print("=== Starting DBK Algorithm ===")
  G = graph.copy()
  num_of_atoms = 0
  iteration = 0
  atom_sizes = []
  if len(graph) <= LIMIT:
    print("=== Input Graph Size is Smaller than LIMIT ===")
    num_of_atoms +=1
    print("=== Finished DBK Algorithm ===")
    end_time = time.time()
    elapsed_time = end_time - start_time
    return num_of_atoms, iteration, atom_sizes, elapsed_time
  print("Preprocessing...")
  graph = remove_zero_degree_nodes(graph)
  k = mc_lower_bound(graph)
  graph = k_core_reduction(graph, len(k))
  if len(graph) == 0:
    end_time = time.time()
    elapsed_time = end_time - start_time
    return num_of_atoms, iteration, atom_sizes, elapsed_time
  if len(graph) <= LIMIT:
    print("=== After K-core Reduction the Graph Size is Smaller than LIMIT ===")
    num_of_atoms += 1
    print("=== Finished DBK Algorithm ===")
    end_time = time.time()
    elapsed_time = end_time - start_time
    return num_of_atoms, iteration, atom_sizes, elapsed_time
  vertex_removal = {graph: []}
  subgraphs = [graph]
  while len(subgraphs) != 0:
    iteration += 1
    SG = subgraphs.pop()
    SG = remove_zero_degree_nodes(SG)
    #print("Current subgraph size:", len(SG))
    assert len(SG) != 0
    vcount = vertex_removal[SG]
    del vertex_removal[SG]
    vertex = lowest_degree_vertex(SG)
    SSG, SG = ch_partitioning(vertex, SG)
    SG = remove_zero_degree_nodes(SG) # BIG
    SSG = remove_zero_degree_nodes(SSG) # SMALL
    SG = k_core_reduction(SG, len(k)-len(vcount)) # 0
    SSG = k_core_reduction(SSG, len(k)-len(vcount+[vertex])) # 1
    vertex_removal[SSG] = vcount+[vertex]
    vertex_removal[SG] = vcount
    #####################################################################################################
    if is_clique(G.subgraph(list(SSG.nodes()))) == True:
      assert is_clique(G.subgraph(list(SSG.nodes())+vertex_removal[SSG])) == True
      if len(SSG)+len(vertex_removal[SSG]) > len(k):
        k = list(SSG.nodes())+vertex_removal[SSG]
      del vertex_removal[SSG]
      SSG = nx.Graph()
    if is_clique(G.subgraph(list(SG.nodes()))) == True:
      assert is_clique(G.subgraph(list(SG.nodes())+vertex_removal[SG])) == True
      if len(SG)+len(vertex_removal[SG]) > len(k):
        k = list(SG.nodes())+vertex_removal[SG]
      del vertex_removal[SG]
      SG = nx.Graph()
    #####################################################################################################
    if len(SSG) != 0:
      SSG_lower = mc_lower_bound(SSG)+vertex_removal[SSG]
      assert is_clique(G.subgraph(SSG_lower)) == True
      #print("SSG lower", len(SSG_lower))
      #print("lowerbound:", len(k))
      if len(SSG_lower) > len(k):
        vcount = vertex_removal[SSG]
        del vertex_removal[SSG]
        k = SSG_lower
        SSG = k_core_reduction(SSG, len(k)-len(vcount))
        SSG = remove_zero_degree_nodes(SSG)
        vertex_removal[SSG] = vcount
      if len(SSG) != 0:
        SSG_upper = mc_upper_bound(SSG)+len(vertex_removal[SSG])
        if SSG_upper > len(k):
          if len(SSG) <= LIMIT:
            #print("=== Terminal Subgraph Found ===")
            #print("Size:", len(SSG))
            atom_sizes.append(len(SSG))
            num_of_atoms += 1
            del vertex_removal[SSG]
          else:
            subgraphs.append(SSG)
        else:
          del vertex_removal[SSG]
    if len(SSG) == 0:
      if SSG in list(vertex_removal.keys()):
        sub_solution_SSG = vertex_removal[SSG]
        del vertex_removal[SSG]
        assert is_clique(G.subgraph(sub_solution_SSG)) == True
        if len(sub_solution_SSG) > len(k):
          k = sub_solution_SSG
    #####################################################################################################
    if len(SG) != 0:
      SG_lower = mc_lower_bound(SG)+vertex_removal[SG]
      assert is_clique(G.subgraph(SG_lower)) == True
      #print("SG lower", len(SG_lower))
      #print("lowerbound:", len(k))
      if len(SG_lower) > len(k):
        vcount = vertex_removal[SG]
        del vertex_removal[SG]
        k = SG_lower
        SG = k_core_reduction(SG, len(k)-len(vcount))
        SG = remove_zero_degree_nodes(SG)
        vertex_removal[SG] = vcount
      if len(SG) != 0:
        SG_upper = mc_upper_bound(SG)+len(vertex_removal[SG])
        if SG_upper > len(k):
          if len(SG) <= LIMIT:
            #print("=== Terminal Subgraph Found ===")
            #print("Size:", len(SG))
            atom_sizes.append(len(SG))
            num_of_atoms += 1
            # sub_solution_SG = solver_function(SG)+vertex_removal[SG]
            del vertex_removal[SG]
          else:
            subgraphs.append(SG)
        else:
          del vertex_removal[SG]
    if len(SG) == 0:
      if SG in list(vertex_removal.keys()):
        sub_solution_SG = vertex_removal[SG]
        del vertex_removal[SG]
        assert is_clique(G.subgraph(sub_solution_SG)) == True
        if len(sub_solution_SG) > len(k):
          k = sub_solution_SG
  assert len(vertex_removal) == 0
  print("=== Finished DBK Algorithm ===")
  end_time = time.time()
  elapsed_time = end_time - start_time
  return num_of_atoms, iteration, atom_sizes, elapsed_time

  # DBK2 with the new eval functions

def DBK_model(pygraph, rank_model, LIMIT=40):
  """
  INPUT:
    - "graph" must be a Networkx Undirected Graph
    - "LIMIT" is an integer describing the largest size of graph which solver_func can solve; all subgraph sizes solved will be less than or equal to LIMIT
    - "solver_function" takes a Networkx Graph, and outputs a list of nodes which are hopefully the Maximum Clique elements; it can be an approximate or exact solver function
  OUTPUT:
    - "k" is a list of graph nodes which form a clique in the input graph. If the solver is exact, then k is the Maximum Clique
  NOTES:
    - The central idea of using bounds is that we maintain a global lower bound on the Maximum Clique. Then, for each sub problem we calculate a fast upper bound.
      If any sub problem has an upper bound which is less than or equal to the global lower bound, we can remove that sub problem from consideration in the remaining iterations of the algorithm.
    - This algorithm does not necessarily enumerate all cliques nor all Maximum Cliques. In particular, it is designed to return a single maximum clique assuming the solver is exact.
      However, the algorithm could be modified to include all maximum cliques found from solving each sub-problem.
    - There are many assert statements in this function. These all serve as "sanity checks"; if any of them are tripped, something went wrong or an input was incorrect
  """
  start_time = time.time()
  features = pygraph.x
  print("all features:", features.shape)
  graph = to_networkx(pygraph)
  assert type(graph) is nx.Graph
  assert type(LIMIT) is int
  assert len(graph) != 0
  print("=== Starting DBK Algorithm ===")
  G = graph.copy()
  num_of_atoms = 0
  iteration = 0
  atom_sizes = []
  if len(graph) <= LIMIT:
    print("=== Input Graph Size is Smaller than LIMIT ===")
    num_of_atoms +=1
    print("=== Finished DBK Algorithm ===")
    end_time = time.time()
    elapsed_time = end_time - start_time
    return num_of_atoms, iteration, atom_sizes, elapsed_time
  print("Preprocessing...")
  graph = remove_zero_degree_nodes(graph)
  k = mc_lower_bound(graph)
  graph = k_core_reduction(graph, len(k))
  if len(graph) == 0:
    end_time = time.time()
    elapsed_time = end_time - start_time
    return num_of_atoms, iteration, atom_sizes, elapsed_time
  if len(graph) <= LIMIT:
    print("=== After K-core Reduction the Graph Size is Smaller than LIMIT ===")
    num_of_atoms += 1
    print("=== Finished DBK Algorithm ===")
    end_time = time.time()
    elapsed_time = end_time - start_time
    return num_of_atoms, iteration, atom_sizes, elapsed_time
  vertex_removal = {graph: []}
  subgraphs = [graph]
  while len(subgraphs) != 0:
    iteration += 1
    #print("Iteration number:", iteration)
    SG = subgraphs.pop()
    SG = remove_zero_degree_nodes(SG)
    #print("Current subgraph size:", len(SG))
    assert len(SG) != 0
    vcount = vertex_removal[SG]
    del vertex_removal[SG]
    SG_copy = SG.copy()
    new_py = graph_to_pyg_data(SG_copy)
    # node names
    node_names = torch.unique(new_py.edge_index)
    node_names_list = node_names.tolist()
    num_nodes = new_py.num_nodes
    new_node_names = torch.arange(0, num_nodes)
    node_mapping = dict(zip(node_names.numpy(), new_node_names.numpy()))
    new_py.edge_index = torch.tensor([[node_mapping[edge[0].item()], node_mapping[edge[1].item()]] for edge in new_py.edge_index.t()])
    new_py.edge_index = new_py.edge_index.t()
    # node features
    new_features = features[node_names_list]
    new_py.x = new_features
    # Get the list of unique node indices (node names)
    vertex = model_vertex_selection(rank_model, new_py)
    vertex = list(SG_copy.nodes)[vertex]
    SSG, SG = ch_partitioning(vertex, SG)
    SG = remove_zero_degree_nodes(SG) # BIG
    SSG = remove_zero_degree_nodes(SSG) # SMALL
    SG = k_core_reduction(SG, len(k)-len(vcount)) # 0
    SSG = k_core_reduction(SSG, len(k)-len(vcount+[vertex])) # 1
    vertex_removal[SSG] = vcount+[vertex]
    vertex_removal[SG] = vcount
    #####################################################################################################
    if is_clique(G.subgraph(list(SSG.nodes()))) == True:
      assert is_clique(G.subgraph(list(SSG.nodes())+vertex_removal[SSG])) == True
      if len(SSG)+len(vertex_removal[SSG]) > len(k):
        k = list(SSG.nodes())+vertex_removal[SSG]
      del vertex_removal[SSG]
      SSG = nx.Graph()
    if is_clique(G.subgraph(list(SG.nodes()))) == True:
      assert is_clique(G.subgraph(list(SG.nodes())+vertex_removal[SG])) == True
      if len(SG)+len(vertex_removal[SG]) > len(k):
        k = list(SG.nodes())+vertex_removal[SG]
      del vertex_removal[SG]
      SG = nx.Graph()
    #####################################################################################################
    if len(SSG) != 0:
      SSG_lower = mc_lower_bound(SSG)+vertex_removal[SSG]
      assert is_clique(G.subgraph(SSG_lower)) == True
      if len(SSG_lower) > len(k):
        vcount = vertex_removal[SSG]
        del vertex_removal[SSG]
        k = SSG_lower
        SSG = k_core_reduction(SSG, len(k)-len(vcount))
        SSG = remove_zero_degree_nodes(SSG)
        vertex_removal[SSG] = vcount
      if len(SSG) != 0:
        SSG_upper = mc_upper_bound(SSG)+len(vertex_removal[SSG])
        if SSG_upper > len(k):
          if len(SSG) <= LIMIT:
            #print("=== Terminal Subgraph Found ===")
            #print("Size:", len(SSG))
            atom_sizes.append(len(SSG))
            num_of_atoms += 1
            del vertex_removal[SSG]
          else:
            subgraphs.append(SSG)
        else:
          del vertex_removal[SSG]
    if len(SSG) == 0:
      if SSG in list(vertex_removal.keys()):
        sub_solution_SSG = vertex_removal[SSG]
        del vertex_removal[SSG]
        assert is_clique(G.subgraph(sub_solution_SSG)) == True
        if len(sub_solution_SSG) > len(k):
          k = sub_solution_SSG
    #####################################################################################################
    if len(SG) != 0:
      SG_lower = mc_lower_bound(SG)+vertex_removal[SG]
      assert is_clique(G.subgraph(SG_lower)) == True
      if len(SG_lower) > len(k):
        vcount = vertex_removal[SG]
        del vertex_removal[SG]
        k = SG_lower
        SG = k_core_reduction(SG, len(k)-len(vcount))
        SG = remove_zero_degree_nodes(SG)
        vertex_removal[SG] = vcount
      if len(SG) != 0:
        SG_upper = mc_upper_bound(SG)+len(vertex_removal[SG])
        if SG_upper > len(k):
          if len(SG) <= LIMIT:
            #print("=== Terminal Subgraph Found ===")
            #print("Size:", len(SG))
            atom_sizes.append(len(SG))
            num_of_atoms += 1
            # sub_solution_SG = solver_function(SG)+vertex_removal[SG]
            del vertex_removal[SG]
          else:
            subgraphs.append(SG)
        else:
          del vertex_removal[SG]
    if len(SG) == 0:
      if SG in list(vertex_removal.keys()):
        sub_solution_SG = vertex_removal[SG]
        del vertex_removal[SG]
        assert is_clique(G.subgraph(sub_solution_SG)) == True
        if len(sub_solution_SG) > len(k):
          k = sub_solution_SG
  assert len(vertex_removal) == 0
  print("=== Finished DBK Algorithm ===")
  end_time = time.time()
  elapsed_time = end_time - start_time
  return num_of_atoms, iteration, atom_sizes, elapsed_time

import random
import numpy as np
random.seed=1
nxgraph = nx.gnp_random_graph(8000, 0.5)
nx.draw(nxgraph)
num_of_atoms2, iteration2, atom_sizes2, time2 = DBK(nxgraph, LIMIT=100)
print(time2)

def model_vertex_selection(model, pygraph):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    model.eval()
    best_node = None
    best_value = -9999
    # get labels
    pygraph.cuda()
    outputs = model(pygraph)
    temp = outputs.detach().cpu()
    outputs = temp.numpy().flatten()
    best_value = -9999
    best_node = None
    for node, output_value in enumerate(outputs):
        if output_value > best_value:
            best_value = output_value
            best_node = node
    return best_node

import numpy as np
loaded_dataset = torch.load('/content/drive/MyDrive/Colab Notebooks/Decomp/LabeledData/test_dataset.pt')

import numpy as np
from tqdm.auto import tqdm

num_of_atoms_bc = []
iteration_bc = []
time_bc = []

num_of_atoms_dbk = []
iteration_dbk = []
time_dbk = []

num_of_atoms_simple = []
iteration_simple = []
time_simple = []

num_of_atoms_rank = []
iteration_rank = []
time_rank = []

gcn_rank_model = GCNRankNet(7, 64)
gcn_rank_model.cuda()
gcn_rank_model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/Decomp/GCNRank_9.pth', map_location=torch.device('cpu')))
bc_model = BinaryClassificationModel(7, 128)
bc_model.cuda()
bc_model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/Decomp//GCNBC_9.pth', map_location=torch.device('cpu')))
simple_model = SimpleRankModel(7, 128)
simple_model.cuda()
simple_model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/Decomp/simplerank_33.pth', map_location=torch.device('cpu')))

for i in tqdm(range(len(loaded_dataset))):
    pygraph = loaded_dataset[i]
    print(i)
    nxgraph = to_networkx(pygraph)
    num_of_atoms1, iteration1, atom_sizes1, time1 = DBK_model(pygraph, bc_model, LIMIT=40)
    num_of_atoms_bc.append(num_of_atoms1)
    iteration_bc.append(iteration1)
    time_bc.append(time1)
    num_of_atoms2, iteration2, atom_sizes2, time2 = DBK(nxgraph, LIMIT=40)
    num_of_atoms_dbk.append(num_of_atoms2)
    iteration_dbk.append(iteration2)
    time_dbk.append(time2)
    num_of_atoms3, iteration3, atom_sizes3, time3 = DBK_model(pygraph, simple_model, LIMIT=40)
    num_of_atoms_simple.append(num_of_atoms3)
    iteration_simple.append(iteration3)
    time_simple.append(time3)
    num_of_atoms4, iteration4, atom_sizes4, time4 = DBK_model(pygraph, gcn_rank_model, LIMIT=40)
    num_of_atoms_rank.append(num_of_atoms4)
    iteration_rank.append(iteration4)
    time_rank.append(time4)

np.save('/content/drive/MyDrive/Colab Notebooks/Decomp/Results/num_of_atoms_bc.npy', num_of_atoms_bc)
np.save('/content/drive/MyDrive/Colab Notebooks/Decomp/Results/iteration_bc.npy', iteration_bc)
np.save('/content/drive/MyDrive/Colab Notebooks/Decomp/Results/time_bc.npy', time_bc)
np.save('/content/drive/MyDrive/Colab Notebooks/Decomp/Results/num_of_atoms_dbk.npy', num_of_atoms_dbk)
np.save('/content/drive/MyDrive/Colab Notebooks/Decomp/Results/iteration_dbk.npy', iteration_dbk)
np.save('/content/drive/MyDrive/Colab Notebooks/Decomp/Results/time_dbk.npy', time_dbk)
np.save('/content/drive/MyDrive/Colab Notebooks/Decomp/Results/num_of_atoms_simple.npy', num_of_atoms_simple)
np.save('/content/drive/MyDrive/Colab Notebooks/Decomp/Results/iteration_simple.npy', iteration_simple)
np.save('/content/drive/MyDrive/Colab Notebooks/Decomp/Results/time_simple.npy', time_simple)
np.save('/content/drive/MyDrive/Colab Notebooks/Decomp/Results/num_of_atoms_rank.npy', num_of_atoms_rank)
np.save('/content/drive/MyDrive/Colab Notebooks/Decomp/Results/iteration_rank.npy', iteration_rank)
np.save('/content/drive/MyDrive/Colab Notebooks/Decomp/Results/time_rank.npy', time_rank)
num_of_atoms_list = [num_of_atoms_bc, num_of_atoms_dbk, num_of_atoms_simple, num_of_atoms_rank]
iteration_list = [iteration_bc, iteration_dbk, iteration_simple, iteration_rank]
time_list = [time_bc, time_dbk, time_simple, time_rank]
print(num_of_atoms_list)
print(iteration_list)
print(time_list)

# Create a boxplot
plt.boxplot(iteration_list, labels=['Binary Classification', 'Min Degree', 'SimpleRank', "Rank"])
plt.title('Boxplot of Number of Iterations')
plt.ylabel('Iterations')
# Show the plot
plt.show()
plt.boxplot(num_of_atoms_list, labels=['Binary Classification', 'Min Degree', 'SimpleRank', "Rank"])
plt.title('Boxplot of Number of Subgraphs')
plt.ylabel('Subgraphs')
# Show the plot
plt.show()
plt.boxplot(time_list, labels=['Binary Classification', 'Min Degree', 'SimpleRank', "Rank"])
plt.title('Boxplot of Time Spent')
plt.ylabel('Time')
# Show the plot
plt.show()

print(iteration_list)
print(num_of_atoms_list)
print(time_list)