# -*- coding: utf-8 -*-
"""AllModelsCode

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YLPLllSKvYAJO6_0okRu1DcVmNrvA01O
"""

!pip install torch torchvision torchaudio
!pip install torch-geometric

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
from torch_geometric.datasets import GNNBenchmarkDataset
import torch_geometric.nn as geo_nn
from torch_geometric.data import DataLoader
import torch_geometric.transforms as T
from torch_geometric.nn import GCNConv, global_mean_pool
from torch_geometric.data import Data
from torch_geometric.transforms import Constant
from tqdm.auto import tqdm
import networkx as nx
import matplotlib.pyplot as plt
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

import torch
from torch_geometric.datasets import GNNBenchmarkDataset
import torch.nn as nn
import torch.optim as optim
import torch_geometric.nn as geo_nn
from torch_geometric.data import DataLoader
import networkx as nx
import torch_geometric.transforms as T
from torch_geometric.nn import GCNConv, global_mean_pool
from torch_geometric.data import Data
from torch_geometric.transforms import Constant
from tqdm.auto import tqdm
from itertools import combinations

# Defining some data preprocessing functions
def graph_to_pyg_data(graph):
    edge_index = torch.tensor(list(graph.edges)).t().contiguous()
    x = torch.eye(graph.number_of_nodes())  # Node features, identity matrix in this example
    y = torch.tensor(list(nx.get_node_attributes(graph, 'label').values()))  # Node labels

    return Data(x=x, edge_index=edge_index, y=y)

def to_networkx(data):
    edge_index = data.edge_index.cpu().numpy()
    edge_attr = None
    if data.edge_attr is not None:
        edge_attr = data.edge_attr.cpu().numpy()

    G = nx.Graph()
    G.add_nodes_from(range(data.num_nodes))
    G.add_edges_from(edge_index.T)

    if edge_attr is not None:
        for i, (src, tgt) in enumerate(edge_index.T):
            G[src][tgt]['edge_attr'] = edge_attr[i]

    return G

    # min max normalize [0, 1]
def min_max_normalize(data, new_min=0, new_max=1):
    # Find the min and max values in the data
    min_val = min(data)
    max_val = max(data)
    if min_val == max_val:
      return [1 for _ in range(len(data))]
    normalized_data = [(x - min_val) / (max_val - min_val) * (new_max - new_min) + new_min for x in data]
    return normalized_data

# top k% of values are 1 rest are 0
def top_k(label_list, k):
    k = round(k*len(label_list))
    input_list = min_max_normalize(label_list)
    # Sort the list in descending order
    sorted_list = sorted(input_list, reverse=True)
    # Determine the threshold index
    threshold_index = min(k, len(sorted_list))
    # Set the first k elements to 1 and the rest to 0
    thresholded_list = [1 if i < threshold_index else 0 for i in range(len(sorted_list))]
    # Create a mapping from original indices to sorted indices
    index_mapping = {original: sorted_index for sorted_index, original in enumerate(sorted(range(len(input_list)), key=lambda x: input_list[x], reverse=True))}
    # Sort the thresholded list back to the original order
    thresholded_list_original_order = [thresholded_list[index_mapping[i]] for i in range(len(input_list))]

    return thresholded_list_original_order

# continuous [0,1]
def continuous(label_list, k):
  label_list = min_max_normalize(label_list)
  return label_list

# value above k are 1 rest are 0
def within_k(label_list, k):
  k = 1-k
  label_list = min_max_normalize(label_list)
  data = [1 if x > k else 0 for x in label_list]
  return data


def max_normalize_binary(label_list, k):
  k = 1-k
  # normalize data
  max_val = max(label_list)
  normalized_data = [(x / max_val) for x in label_list]
  data = [1 if x > k else 0 for x in normalized_data]
  return data

def max_normalize(label_list, k):
  # normalize data
  max_val = max(label_list)
  normalized_data = [(x / max_val) for x in label_list]
  return normalized_data

# Normalize dataset function
def dataset_normalize(dataset, normalize_function, normalize_param):
  for graph_idx in tqdm(range(len(dataset))):
    data = dataset[graph_idx]
    node_labels_list = data.y.tolist()
    normalized_node_labels_list = normalize_function(node_labels_list, normalize_param)
    data.y = torch.tensor(normalized_node_labels_list)
  return dataset

# get normalized labels
loaded_dataset = torch.load('/content/drive/MyDrive/Colab Notebooks/Decomp/LabeledData/train_dataset.pt')
normalize_function = max_normalize_binary
normalize_param = 0.2 # top 20% of nodes are 1 rest are 0
dataset_normalize(loaded_dataset, normalize_function=normalize_function, normalize_param=normalize_param)
labels = torch.empty(0, dtype=torch.float32)
features = torch.empty(0, dtype=torch.float32)
for graph in loaded_dataset:
    temp_labels = graph.y
    labels = torch.cat([labels, temp_labels], dim=0)
    temp_features = graph.x
    features = torch.cat([features, temp_features], dim=0)

print(labels.shape)
print(features.shape)

train_size = int(round(0.8 * len(loaded_dataset)))
print(train_size)
print(len(loaded_dataset) - train_size)
train_graphs = loaded_dataset[:train_size]
val_graphs = loaded_dataset[train_size:]

"""# BINARY CLASSIFICATION"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
from torch_geometric.nn import GCNConv
import torch.nn.functional as F

class GCNLayer(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(GCNLayer, self).__init__()
        self.gcn = GCNConv(input_size, hidden_size)

    def forward(self, x, edge_index):
        return self.gcn(x, edge_index)

class PairwiseRankingDataset(Dataset):
    def __init__(self, features, labels):
        self.features = features
        self.labels = labels

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

class BinaryClassificationModel(nn.Module):
    def __init__(self, input_size, gcn_hidden_size, dropout_prob=0.3):
        super(BinaryClassificationModel, self).__init__()
        # GCN layers
        self.gcn1 = GCNLayer(input_size, gcn_hidden_size)
        self.gcn2 = GCNLayer(gcn_hidden_size, gcn_hidden_size)

        self.fc1 = nn.Linear(gcn_hidden_size, 64)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 1)
        self.sigmoid = nn.Sigmoid()
        self.dropout = nn.Dropout(p=dropout_prob)
        self.batch_norm1 = nn.BatchNorm1d(64)
        self.batch_norm2 = nn.BatchNorm1d(32)
        self.batch_norm3 = nn.BatchNorm1d(1)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        # GCN layers
        x = self.gcn1(x, edge_index)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.gcn2(x, edge_index)
        x = self.relu(x)
        x = self.dropout(x)

        # MLP layers
        x = self.batch_norm1(self.fc1(x))
        x = self.relu(x)
        x = self.dropout(x)
        x = self.batch_norm2(self.fc2(x))
        x = self.relu(x)
        x = self.dropout(x)
        x = self.batch_norm3(self.fc3(x))
        # x = self.sigmoid(x)
        return x

def pairwise_ranking_loss(scores, labels):
    # Calculate pairwise differences only for positive pairs (labels == 1)
    pairwise_diff = scores[labels == 1].view(-1, 1) - scores[labels == 0].view(1, -1)
    return torch.mean(nn.functional.logsigmoid(-pairwise_diff))

class FocalLoss(nn.Module):
    def __init__(self, alpha=0.9, gamma=2, logits=True, reduce=True):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.logits = logits
        self.reduce = reduce

    def forward(self, inputs, targets):
        if self.logits:
            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        else:
            BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='none')

        pt = torch.exp(-BCE_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss

        if self.reduce:
            return torch.mean(focal_loss)
        else:
            return focal_loss

# Function to calculate accuracy
def calculate_accuracy(outputs, labels):
    # Assuming your task is binary classification
    predictions = torch.sigmoid(outputs) > 0.5
    TP = ((predictions == 1) & (labels == 1)).sum().item()
    FP = ((predictions == 1) & (labels == 0)).sum().item()
    if TP + FP == 0:
        TPR = 1
    else:
        TPR = TP / (TP + FP)
    correct = (predictions == labels).sum().item()
    total = labels.size(0)
    accuracy = correct / total
    return accuracy, TPR


def train_model(model, train_loader, val_loader, optimizer, criterion, batch_size, epochs=10):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    criterion = criterion.to(device)
    train_losses = []
    val_losses = []
    train_accuracies = []
    val_accuracies = []
    val_tprs = []
    train_tprs = []
    base_path = '/content/drive/MyDrive/Colab Notebooks/Decomp/Models/GCNBC'
    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        start = 0
        train_epoch_loss = 0.0
        train_epoch_accuracy = 0.0
        train_epoch_tprs = 0.0
        total_length = len(train_loader) // batch_size
        for i in range(start, len(train_loader), batch_size):
            optimizer.zero_grad()
            scores = torch.empty(0, dtype=torch.float32, requires_grad=True).to(device)
            labels = torch.empty(0, dtype=torch.float32, requires_grad=True).to(device)
            for j in range(start, start + batch_size):
                if j >= len(train_loader):
                    break
                graph = train_loader[j]
                temp = model(graph)
                scores = torch.cat([scores, temp], dim=0)
                temp = graph.y
                labels = torch.cat([labels, temp], dim=0)
            start += batch_size
            scores = scores.squeeze().to(device)
            loss = criterion(scores, labels)
            train_epoch_loss += loss.item()

            # Calculate accuracy
            accuracy,tpr = calculate_accuracy(scores, labels)
            train_epoch_tprs += tpr
            train_epoch_accuracy += accuracy

            # Backward pass and optimization
            loss.backward()
            optimizer.step()
        train_tprs.append(train_epoch_tprs / total_length)
        average_train_loss = train_epoch_loss / total_length
        average_train_accuracy = train_epoch_accuracy / total_length
        train_losses.append(average_train_loss)
        train_accuracies.append(average_train_accuracy)

        # Validation
        model.eval()
        val_epoch_loss = 0.0
        val_epoch_accuracy = 0.0
        val_epoch_tpr = 0.0
        test_length = len(val_loader) // batch_size
        with torch.no_grad():
            start = 0
            for i in range(start, len(val_loader), batch_size):
                graph.to(device)
                optimizer.zero_grad()
                scores = torch.empty(0, dtype=torch.float32, requires_grad=True).to(device)
                labels = torch.empty(0, dtype=torch.float32, requires_grad=True).to(device)
                for j in range(start, start + batch_size):
                    if j >= len(val_loader):
                        break
                    graph = val_loader[j]
                    scores = torch.cat([scores, model(graph)], dim=0)
                    labels = torch.cat([labels, graph.y], dim=0)
                start += batch_size
                scores = scores.squeeze().to(device)
                loss = criterion(scores, labels)
                val_epoch_loss += loss.item()

                # Calculate accuracy
                accuracy, tpr = calculate_accuracy(scores, labels)
                val_epoch_accuracy += accuracy
                val_epoch_tpr += tpr


        val_tprs.append(val_epoch_tpr / test_length)
        average_val_loss = val_epoch_loss / test_length
        average_val_accuracy = val_epoch_accuracy / test_length
        val_losses.append(average_val_loss)
        val_accuracies.append(average_val_accuracy)
        save_path = f'{base_path}_{epoch}.pth'
        torch.save(model.state_dict(), save_path)

        # Print progress
        print(f'Epoch [{epoch+1}/{epochs}], '
              f'Train Loss: {average_train_loss:.4f}, Train Acc: {average_train_accuracy:.4f}, '
              f'Train TPR: {train_epoch_tprs / total_length:.4f}, '
              f'Val Loss: {average_val_loss:.4f}, Val Acc: {average_val_accuracy:.4f}, '
              f'Val TPR: {val_epoch_tpr / test_length:.4f}')
    # Plot the training and validation loss curves
    plt.plot(range(1, epochs + 1), train_losses, label='Training Loss')
    plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()


batch_size = 2
epochs = 30
input_size = 7
gcn_hidden_size = 128
bc_model = BinaryClassificationModel(input_size, gcn_hidden_size, dropout_prob=0)


criterion = FocalLoss()
optimizer = optim.Adam(bc_model.parameters(), lr=0.008)

# Train the model with validation
train_model(bc_model, train_graphs, val_graphs, optimizer, criterion, batch_size=batch_size, epochs=epochs)

"""# SIMPLE RANK"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
from torch_geometric.nn import GCNConv
import torch.nn.functional as F

class GCNLayer(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(GCNLayer, self).__init__()
        self.gcn = GCNConv(input_size, hidden_size)

    def forward(self, x, edge_index):
        return self.gcn(x, edge_index)

class PairwiseRankingDataset(Dataset):
    def __init__(self, features, labels):
        self.features = features
        self.labels = labels

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

class SimpleRankModel(nn.Module):
    def __init__(self, input_size, gcn_hidden_size, dropout_prob=0.3):
        super(SimpleRankModel, self).__init__()
        # GCN layers
        self.gcn1 = GCNLayer(input_size, gcn_hidden_size)
        self.gcn2 = GCNLayer(gcn_hidden_size, gcn_hidden_size)

        self.fc1 = nn.Linear(gcn_hidden_size, 64)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 1)
        self.sigmoid = nn.Sigmoid()
        self.dropout = nn.Dropout(p=dropout_prob)
        self.batch_norm1 = nn.BatchNorm1d(64)
        self.batch_norm2 = nn.BatchNorm1d(32)
        self.batch_norm3 = nn.BatchNorm1d(1)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        # GCN layers
        x = self.gcn1(x, edge_index)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.gcn2(x, edge_index)
        x = self.relu(x)
        x = self.dropout(x)

        # MLP layers
        x = self.batch_norm1(self.fc1(x))
        x = self.relu(x)
        x = self.dropout(x)
        x = self.batch_norm2(self.fc2(x))
        x = self.relu(x)
        x = self.dropout(x)
        x = self.batch_norm3(self.fc3(x))
        # x = self.sigmoid(x)
        return x

def pairwise_ranking_loss(scores, labels):
    # Calculate pairwise differences only for positive pairs (labels == 1)
    pairwise_diff = scores[labels == 1].view(-1, 1) - scores[labels == 0].view(1, -1)
    return -torch.mean(nn.functional.logsigmoid(pairwise_diff))

# Function to calculate accuracy
def calculate_accuracy(outputs, labels):
    # Assuming your task is binary classification
    predictions = torch.sigmoid(outputs) > 0.5
    TP = ((predictions == 1) & (labels == 1)).sum().item()
    FP = ((predictions == 1) & (labels == 0)).sum().item()
    if TP + FP == 0:
        TPR = 1
    else:
        TPR = TP / (TP + FP)
    correct = (predictions == labels).sum().item()
    total = labels.size(0)
    accuracy = correct / total
    return accuracy, TPR


def train_model(model, train_loader, val_loader, optimizer, criterion, batch_size, epochs=10):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    train_losses = []
    val_losses = []
    train_accuracies = []
    val_accuracies = []
    val_tprs = []
    train_tprs = []
    base_path = '/content/drive/MyDrive/Colab Notebooks/Decomp/Models/simplerank'
    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        start = 0
        train_epoch_loss = 0.0
        train_epoch_accuracy = 0.0
        train_epoch_tprs = 0.0
        total_length = len(train_loader) // batch_size
        for i in range(start, len(train_loader), batch_size):
            optimizer.zero_grad()
            scores = torch.empty(0, dtype=torch.float32, requires_grad=True).to(device)
            labels = torch.empty(0, dtype=torch.float32, requires_grad=True).to(device)
            for j in range(start, start + batch_size):
                if j >= len(train_loader):
                    break
                graph = train_loader[j]
                temp = model(graph)
                scores = torch.cat([scores, temp], dim=0)
                temp = graph.y
                labels = torch.cat([labels, temp], dim=0)
            start += batch_size
            scores = scores.squeeze().to(device)
            loss = criterion(scores, labels)
            train_epoch_loss += loss.item()

            # Calculate accuracy
            accuracy,tpr = calculate_accuracy(scores, labels)
            train_epoch_tprs += tpr
            train_epoch_accuracy += accuracy

            # Backward pass and optimization
            loss.backward()
            optimizer.step()
        train_tprs.append(train_epoch_tprs / total_length)
        average_train_loss = train_epoch_loss / total_length
        average_train_accuracy = train_epoch_accuracy / total_length
        train_losses.append(average_train_loss)
        train_accuracies.append(average_train_accuracy)

        # Validation
        model.eval()
        val_epoch_loss = 0.0
        val_epoch_accuracy = 0.0
        val_epoch_tpr = 0.0
        test_length = len(val_loader) // batch_size
        with torch.no_grad():
            start = 0
            for i in range(start, len(val_loader), batch_size):
                graph.to(device)
                optimizer.zero_grad()
                scores = torch.empty(0, dtype=torch.float32, requires_grad=True).to(device)
                labels = torch.empty(0, dtype=torch.float32, requires_grad=True).to(device)
                for j in range(start, start + batch_size):
                    if j >= len(val_loader):
                        break
                    graph = val_loader[j]
                    scores = torch.cat([scores, model(graph)], dim=0)
                    labels = torch.cat([labels, graph.y], dim=0)
                start += batch_size
                scores = scores.squeeze().to(device)
                loss = criterion(scores, labels)
                val_epoch_loss += loss.item()

                # Calculate accuracy
                accuracy, tpr = calculate_accuracy(scores, labels)
                val_epoch_accuracy += accuracy
                val_epoch_tpr += tpr


        val_tprs.append(val_epoch_tpr / test_length)
        average_val_loss = val_epoch_loss / test_length
        average_val_accuracy = val_epoch_accuracy / test_length
        val_losses.append(average_val_loss)
        val_accuracies.append(average_val_accuracy)
        save_path = f'{base_path}_{epoch+1}.pth'
        torch.save(model.state_dict(), save_path)

        # Print progress
        print(f'Epoch [{epoch+1}/{epochs}], '
              f'Train Loss: {average_train_loss:.4f}, Train Acc: {average_train_accuracy:.4f}, '
              f'Train TPR: {train_epoch_tprs / total_length:.4f}, '
              f'Val Loss: {average_val_loss:.4f}, Val Acc: {average_val_accuracy:.4f}, '
              f'Val TPR: {val_epoch_tpr / test_length:.4f}')
    # Plot the training and validation loss curves
    plt.plot(range(1, epochs + 1), train_losses, label='Training Loss')
    plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()


batch_size = 1
epochs = 50
input_size = 7
gcn_hidden_size = 64
simple_rank_model = SimpleRankModel(input_size, gcn_hidden_size)


criterion = pairwise_ranking_loss
optimizer = optim.Adam(simple_rank_model.parameters(), lr=0.005)

# Train the model with validation
train_model(simple_rank_model, train_graphs, val_graphs, optimizer, criterion, batch_size=batch_size, epochs=epochs)

"""# PairwiseRank

"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
from torch_geometric.nn import GCNConv
import torch.nn.functional as F

class GCNLayer(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(GCNLayer, self).__init__()
        self.gcn = GCNConv(input_size, hidden_size)

    def forward(self, x, edge_index):
        return self.gcn(x, edge_index)


class PairwiseRankingDataset(Dataset):
    def __init__(self, features, labels):
        self.features = features
        self.labels = labels

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

class GCNRankNet(nn.Module):
    def __init__(self, input_size, gcn_hidden_size, dropout_prob=0.3):
        super(GCNRankNet, self).__init__()
        # GCN layers
        self.gcn1 = GCNLayer(input_size, gcn_hidden_size)
        self.gcn2 = GCNLayer(gcn_hidden_size, gcn_hidden_size)

        self.fc1 = nn.Linear(gcn_hidden_size, 64)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 1)
        self.sigmoid = nn.Sigmoid()
        self.dropout = nn.Dropout(p=dropout_prob)
        self.batch_norm1 = nn.BatchNorm1d(64)
        self.batch_norm2 = nn.BatchNorm1d(32)
        self.batch_norm3 = nn.BatchNorm1d(1)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        # GCN layers
        x = self.gcn1(x, edge_index)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.gcn2(x, edge_index)
        x = self.relu(x)
        x = self.dropout(x)

        # MLP layers
        x = self.batch_norm1(self.fc1(x))
        x = self.relu(x)
        x = self.dropout(x)
        x = self.batch_norm2(self.fc2(x))
        x = self.relu(x)
        x = self.dropout(x)
        x = self.batch_norm3(self.fc3(x))
        # x = self.sigmoid(x)
        return x

def pairwise_ranking_loss(scores, labels):
    # Calculate pairwise differences only for positive pairs (labels == 1)
    pairwise_diff = scores[labels == 1].view(-1, 1) - scores[labels == 0].view(1, -1)
    return -torch.mean(nn.functional.logsigmoid(pairwise_diff))

def train_model(model, train_loader, val_loader, optimizer, criterion, batch_size, epochs=10):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    criterion = criterion.to(device)
    train_losses = []
    val_losses = []
    val_accuracies = []
    base_path = '/content/drive/MyDrive/Colab Notebooks/Decomp/Models/SimpleRank'
    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        start = 0
        for i in range(start, len(train_loader), batch_size):
            optimizer.zero_grad()
            scores = torch.empty(0, dtype=torch.float32, requires_grad=True).to(device)
            labels = torch.empty(0, dtype=torch.float32, requires_grad=True).to(device)
            for j in range(start, start + batch_size):
                if j >= len(train_loader):
                    break
                graph = train_loader[j]
                temp = model(graph)
                scores = torch.cat([scores, temp], dim=0)
                temp = graph.y
                labels = torch.cat([labels, temp], dim=0)
            start += batch_size
            # Create all possible pairs
            pairs_list = list(combinations(range(len(scores)), 2))
            # Calculate pairwise scores
            loss = 0
            pairwise_labels = torch.empty(0, dtype=torch.float32, requires_grad=True)
            pairwise_scores_list = []
            for pairs in pairs_list:
                if labels[pairs[0]] > labels[pairs[1]]:
                    pairwise_label = 1
                elif labels[pairs[0]] < labels[pairs[1]]:
                    pairwise_label = -1
                else:
                    pairwise_label = 0
                pairwise_scores_list.append([scores[pairs[0]], scores[pairs[1]]])
                pairwise_labels = torch.cat((pairwise_labels, torch.tensor([pairwise_label])))
            pairwise_labels = pairwise_labels.to(device)
            pairwise_scores = torch.tensor(pairwise_scores_list, dtype=torch.float32, requires_grad=True).to(device)
            loss = criterion(pairwise_scores[:, 0], pairwise_scores[:, 1], pairwise_labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        average_loss = running_loss / len(train_loader)
        train_losses.append(average_loss)
        print(f"Epoch {epoch + 1}/{epochs}, Training Loss: {average_loss}")

        # Validation phase
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            start = 0
            for i in range(start, len(val_loader), batch_size):
                graph.to(device)
                optimizer.zero_grad()
                scores = torch.empty(0, dtype=torch.float32, requires_grad=True).to(device)
                labels = torch.empty(0, dtype=torch.float32, requires_grad=True).to(device)
                for j in range(start, start + batch_size):
                    if j >= len(val_loader):
                        break
                    graph = val_loader[j]
                    scores = torch.cat([scores, model(graph)], dim=0)
                    labels = torch.cat([labels, graph.y], dim=0)
                start += batch_size
                # Create all possible pairs
                pairs_list = list(combinations(range(len(scores)), 2))
                # Calculate pairwise scores
                loss = 0
                pairwise_labels = torch.empty(0, dtype=torch.float32, requires_grad=True)
                pairwise_scores_list = []
                for pairs in pairs_list:
                    if labels[pairs[0]] > labels[pairs[1]]:
                        pairwise_label = 1
                    elif labels[pairs[0]] < labels[pairs[1]]:
                        pairwise_label = -1
                    else:
                        pairwise_label = 0
                    pairwise_scores_list.append([scores[pairs[0]], scores[pairs[1]]])
                    pairwise_labels = torch.cat((pairwise_labels, torch.tensor([pairwise_label])))
                pairwise_labels = pairwise_labels.to(device)
                pairwise_scores = torch.tensor(pairwise_scores_list, dtype=torch.float32, requires_grad=True).to(device)
                loss = criterion(pairwise_scores[:, 0], pairwise_scores[:, 1], pairwise_labels)
                val_loss += loss.item()
                acc = 0
                for i in range(len(pairwise_scores)):
                    if pairwise_scores[i][0] > pairwise_scores[i][1]:
                        if pairwise_labels[i] == 1:
                            acc += 1
                    elif pairwise_scores[i][0] < pairwise_scores[i][1]:
                        if pairwise_labels[i] == -1:
                            acc += 1
                    else:
                        if pairwise_labels[i] == 0:
                            acc += 1
                accuracy = acc / len(pairwise_scores)
                val_accuracies.append(accuracy)

        average_val_loss = val_loss / len(val_loader)
        val_losses.append(average_val_loss)
        print(f"Validation Loss: {average_val_loss}")
        print(f"Validation Accuracy: {accuracy}")
        save_path = f'{base_path}_{epoch}.pth'
        torch.save(model.state_dict(), save_path)
    # Plot the training and validation loss curves
    plt.plot(range(1, epochs + 1), train_losses, label='Training Loss')
    plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Assuming you have your features and labels as torch tensors
# features and labels should be of size (num_samples, 20) and (num_samples,) respectively

# Create a PairwiseRankingDataset
# dataset = PairwiseRankingDataset(features, labels)

# Split the dataset into training and validation sets
# train_size = int(0.8 * len(dataset))
# val_size = len(dataset) - train_size
# train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])

# Create DataLoader instances
batch_size = 8
epochs = 1
# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# Create the RankNet model
input_size = 7
gcn_hidden_size = 64
gcn_rank_model = GCNRankNet(input_size, gcn_hidden_size, dropout_prob=0)

# Binary Cross Entropy Loss and Adam optimizer

# criterion = nn.BCEWithLogitsLoss()
# criterion = pairwise_ranking_loss
criterion = nn.MarginRankingLoss(reduction='sum')
optimizer = optim.Adam(gcn_rank_model.parameters(), lr=0.005)

# Train the model with validation
train_model(gcn_rank_model, train_graphs, val_graphs, optimizer, criterion, batch_size=batch_size, epochs=epochs)

# function to evaluate imitation algo GCN VERSION
def model_imitation_evaluation(model, pygraph):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    model.eval()
    nxgraph = to_networkx(pygraph)
    best_node = None
    best_value = -9999
    # get labels
    normalize_function = max_normalize # [0,1] continuous
    normalize_param = 0.2
    dataset_normalize([pygraph], normalize_function=normalize_function, normalize_param=normalize_param)
    labels = pygraph.y
    outputs = model(pygraph)
    temp = outputs.detach().cpu()
    outputs = temp.numpy().flatten()
    best_value = -9999
    best_node = None
    for node, output_value in enumerate(outputs):
        if output_value > best_value:
            best_value = output_value
            best_node = node
    output_label = labels[best_node]
    return output_label

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
from torch.utils.data import Subset, random_split
import numpy as np

# Load data and normalize the labels
loaded_dataset = torch.load('/content/drive/MyDrive/Colab Notebooks/Decomp/LabeledData/test_dataset.pt')

test_size = len(loaded_dataset)
gcn_rank_outputs = []
gcn_rank_outputsnoequal = []
bc_outputs = []
simple_rank_outputs = []

for i in tqdm(range(test_size)):
    graph = loaded_dataset[i]
    gcn_rank_model = GCNRankNet(7, 64)
    gcn_rank_model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/Decomp/GCNRank.pth', map_location=torch.device('cpu')))
    gcn_rank_modelnoequal = GCNRankNet(7, 64)
    gcn_rank_modelnoequal.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/Decomp/GCNRankNoEqual.pth', map_location=torch.device('cpu')))
    bc_model = BinaryClassificationModel(7, 128)
    bc_model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/Decomp/GCNBC.pth', map_location=torch.device('cpu')))
    simple_rank_model = SimpleRankModel(7, 128)
    simple_rank_model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/Decomp/simplerank.pth', map_location=torch.device('cpu')))
    gcn_rank_output = model_imitation_evaluation(gcn_rank_model, graph)
    gcn_rank_outputnoequal = model_imitation_evaluation(gcn_rank_modelnoequal, graph)
    bc_output = model_imitation_evaluation(bc_model, graph)
    simple_rank_output = model_imitation_evaluation(simple_rank_model, graph)
    gcn_rank_outputs.append(gcn_rank_output.item())
    gcn_rank_outputsnoequal.append(gcn_rank_outputnoequal.item())
    bc_outputs.append(bc_output.item())
    simple_rank_outputs.append(simple_rank_output.item())

print('\n')
print(gcn_rank_outputs)
print(gcn_rank_outputsnoequal)
print(bc_outputs)
print(simple_rank_outputs)

print(np.mean(gcn_rank_outputs))
print(np.mean(gcn_rank_outputsnoequal))
print(np.mean(bc_outputs))
print(np.mean(simple_rank_outputs))

print(gcn_rank_outputs)
print(gcn_rank_outputsnoequal)
print(bc_outputs)
print(simple_rank_outputs)

print(np.mean(gcn_rank_outputs))
print(np.mean(gcn_rank_outputsnoequal))
print(np.mean(bc_outputs))
print(np.mean(simple_rank_outputs))

bc_outputs = [0.75, 0.4444444477558136, 1.0, 1.0, 1.0, 0.75, 0.39080458879470825, 0.8235294222831726, 0.6000000238418579, 0.800000011920929, 0.5, 1.0, 0.529411792755127, 0.8333333134651184, 0.6666666865348816, 1.0, 0.800000011920929, 0.5, 1.0, 1.0, 0.5, 1.0, 0.04040404036641121, 1.0, 1.0, 1.0, 0.03883495181798935, 1.0, 1.0, 1.0, 0.06521739065647125, 0.8260869383811951, 1.0, 0.01785714365541935, 1.0, 1.0, 1.0, 0.5, 0.5, 0.6666666865348816, 0.5, 0.75, 1.0, 1.0, 0.03846153989434242, 1.0, 0.75, 1.0, 0.8181818127632141, 1.0, 1.0, 0.6666666865348816, 1.0, 0.6666666865348816, 0.800000011920929, 1.0, 1.0, 0.10948905348777771, 0.25974026322364807, 1.0, 1.0, 0.5882353186607361, 0.8181818127632141, 1.0, 0.800000011920929, 0.75, 1.0, 0.06451612710952759, 1.0, 0.033898305147886276, 1.0, 0.5, 0.33636364340782166, 1.0, 0.5, 1.0, 0.3564356565475464, 1.0, 1.0, 0.07619047909975052, 0.800000011920929, 1.0, 1.0, 0.7857142686843872, 0.2525252401828766, 0.6666666865348816, 1.0, 0.5, 0.8333333134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.625, 1.0, 0.5, 0.5, 0.7777777910232544, 0.6666666865348816, 0.75, 0.3333333432674408, 0.8125, 0.4444444477558136, 0.5, 0.5833333134651184, 1.0, 0.625, 0.08791209012269974, 1.0, 1.0, 0.1489361673593521, 0.7272727489471436, 1.0, 1.0, 1.0, 0.5833333134651184, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3461538553237915, 1.0, 0.8148148059844971, 1.0, 1.0, 1.0, 0.75, 0.018691588193178177, 0.7142857313156128, 0.5714285969734192, 1.0, 1.0, 1.0, 1.0, 0.7142857313156128, 0.6000000238418579, 1.0, 1.0, 1.0, 1.0, 0.5714285969734192, 0.6666666865348816, 0.5185185074806213, 1.0, 0.6666666865348816, 0.8333333134651184, 1.0, 0.25, 0.8461538553237915, 1.0, 0.5, 1.0, 1.0, 0.7083333134651184, 1.0, 1.0, 0.4615384638309479, 1.0, 0.5, 0.6363636255264282, 1.0, 0.8333333134651184, 0.5, 1.0, 0.4615384638309479, 0.5151515007019043, 1.0, 0.36000001430511475, 1.0, 0.5, 1.0, 0.78125, 0.1785714328289032, 0.6666666865348816, 1.0, 1.0, 0.6666666865348816, 1.0, 0.3333333432674408, 1.0, 1.0, 0.6666666865348816, 1.0, 0.020618556067347527, 0.12931033968925476, 0.5, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.800000011920929, 0.5, 1.0, 0.03669724613428116, 0.8333333134651184, 0.5, 0.6363636255264282, 0.5714285969734192, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.4000000059604645, 1.0, 0.016260161995887756, 0.5, 0.5714285969734192, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5384615659713745, 1.0, 0.15315315127372742, 1.0, 1.0, 1.0, 0.5, 1.0, 0.75, 0.25, 1.0, 1.0, 0.06451612710952759, 0.5, 1.0, 0.3333333432674408, 0.5333333611488342, 0.75, 1.0, 0.19298245012760162, 0.6666666865348816, 1.0, 1.0, 1.0, 1.0, 0.3333333432674408, 0.7333333492279053, 1.0, 0.6666666865348816, 0.8888888955116272, 0.5714285969734192, 0.20512820780277252, 0.6666666865348816, 0.6666666865348816, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.13333334028720856, 0.6315789222717285, 0.875, 1.0, 1.0, 1.0, 0.4000000059604645, 0.5, 0.6666666865348816, 0.75, 1.0, 0.03773584961891174, 1.0, 1.0, 1.0, 0.6666666865348816, 1.0, 1.0, 0.6666666865348816, 0.6000000238418579, 1.0, 1.0, 1.0, 0.059405941516160965, 0.8333333134651184, 1.0, 0.0181818176060915, 1.0, 1.0, 1.0, 1.0, 0.4285714328289032, 1.0, 0.16239316761493683, 0.75, 1.0, 0.8333333134651184, 1.0, 1.0, 0.04395604506134987, 1.0, 0.5, 0.7692307829856873, 1.0, 0.529411792755127, 0.2110091745853424, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666865348816, 1.0, 0.019999999552965164, 0.5, 0.019999999552965164, 0.020202020183205605, 0.06976744532585144, 1.0, 0.75, 1.0, 1.0, 0.8333333134651184, 0.699999988079071, 0.03883495181798935, 1.0, 1.0, 0.3333333432674408, 1.0, 1.0, 1.0, 0.5, 1.0, 0.3457943797111511, 1.0, 0.692307710647583, 1.0, 0.8888888955116272, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7142857313156128, 1.0, 1.0, 1.0, 0.75, 1.0, 0.3636363744735718, 0.7272727489471436, 0.75, 0.375, 1.0, 1.0, 1.0, 0.4736842215061188, 0.7142857313156128, 0.12621359527111053, 1.0, 0.6363636255264282, 0.6000000238418579, 1.0, 1.0, 0.75, 0.6842105388641357, 0.6000000238418579, 1.0, 1.0, 1.0, 0.875, 1.0, 1.0, 1.0, 1.0, 0.020408162847161293, 1.0, 0.7631579041481018, 0.6000000238418579, 1.0, 1.0, 0.7647058963775635, 1.0, 0.6315789222717285, 0.5, 1.0, 0.6000000238418579, 1.0, 1.0, 1.0, 1.0, 0.3333333432674408, 0.1666666716337204, 0.5, 1.0, 0.1666666716337204, 1.0, 1.0, 1.0, 1.0, 0.800000011920929, 1.0, 0.6000000238418579, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6000000238418579, 0.6666666865348816, 1.0, 1.0, 0.06185567006468773, 1.0, 1.0, 0.8333333134651184, 1.0, 1.0, 1.0, 0.6666666865348816, 0.5, 0.75, 1.0, 1.0, 0.75, 1.0, 0.800000011920929, 1.0, 1.0, 0.5, 1.0, 1.0, 0.625, 1.0, 1.0, 1.0, 1.0, 0.8181818127632141, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5887850522994995, 1.0, 0.7142857313156128, 0.75, 1.0, 1.0, 1.0, 0.8181818127632141, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.3333333432674408, 0.75, 0.75, 0.16296295821666718, 1.0, 1.0, 0.01785714365541935, 1.0, 1.0, 1.0, 1.0, 0.8571428656578064, 1.0, 0.4736842215061188, 1.0, 0.3333333432674408, 1.0, 0.3333333432674408, 1.0, 0.8333333134651184, 1.0, 0.800000011920929, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.800000011920929, 1.0, 1.0, 0.6842105388641357, 0.5, 0.5, 0.6000000238418579, 0.7058823704719543, 1.0, 1.0, 1.0, 0.5199999809265137, 1.0, 1.0, 0.8823529481887817, 1.0, 0.6666666865348816, 1.0, 1.0, 0.800000011920929, 0.1034482792019844, 1.0, 1.0, 1.0, 0.523809552192688, 1.0, 1.0, 0.7142857313156128, 0.05833333358168602, 1.0, 0.75, 1.0, 0.6000000238418579, 1.0, 1.0, 1.0, 0.6666666865348816, 0.6000000238418579, 1.0, 1.0, 1.0, 0.692307710647583, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6363636255264282, 0.5, 0.5, 0.574999988079071, 0.8333333134651184, 0.06818182021379471, 0.5, 1.0, 0.7222222089767456, 1.0, 0.3333333432674408, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.5555555820465088, 1.0, 0.13483145833015442, 1.0, 0.3333333432674408, 1.0, 1.0, 1.0, 0.04210526496171951, 1.0, 1.0, 1.0, 0.11235955357551575, 0.5, 0.6666666865348816, 0.5, 0.5, 0.6000000238418579, 1.0, 0.6000000238418579, 0.10204081982374191, 1.0, 0.7142857313156128, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333134651184, 0.6000000238418579, 1.0, 0.6428571343421936, 1.0, 0.6666666865348816, 0.017241379246115685, 1.0, 1.0, 0.6000000238418579, 0.523809552192688, 0.7647058963775635, 1.0, 0.6000000238418579, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.5, 0.8888888955116272, 0.6666666865348816, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666865348816, 0.1355932205915451, 0.75, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2181818187236786, 0.5, 0.75, 0.6666666865348816, 1.0, 0.2142857164144516, 0.75, 0.6666666865348816, 0.5, 0.75, 1.0, 1.0, 1.0, 1.0, 0.6666666865348816, 0.8181818127632141, 1.0, 0.6000000238418579, 1.0, 1.0, 1.0, 1.0, 0.5, 0.7142857313156128, 1.0, 0.6363636255264282, 0.625, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666865348816, 0.5, 0.4000000059604645, 1.0, 0.75, 0.8333333134651184, 1.0, 0.07999999821186066, 0.6666666865348816, 0.5833333134651184, 1.0, 1.0, 0.6666666865348816, 0.6470588445663452, 1.0, 1.0, 1.0, 0.692307710647583, 0.8888888955116272, 1.0, 1.0, 1.0, 0.6666666865348816, 0.6666666865348816, 0.875, 0.7647058963775635, 0.6666666865348816, 1.0, 1.0, 0.6666666865348816, 0.5714285969734192, 1.0, 0.03361344709992409, 1.0, 0.040816325694322586, 0.11999999731779099, 1.0, 1.0, 0.21374045312404633, 0.018518518656492233, 0.8461538553237915, 0.0416666679084301, 0.75, 1.0, 0.75, 0.37168142199516296, 0.5, 1.0, 1.0, 1.0, 0.17543859779834747, 0.039603959769010544, 0.75, 0.4000000059604645, 0.5, 1.0, 0.800000011920929, 1.0, 0.22033898532390594, 0.6666666865348816, 0.7037037014961243, 0.019801979884505272, 1.0, 0.75, 1.0, 1.0, 1.0, 0.5789473652839661, 0.75, 0.0941176488995552, 1.0, 0.10958904027938843, 0.5, 0.9047619104385376, 1.0, 1.0, 0.4761904776096344, 1.0, 1.0, 0.6666666865348816, 1.0, 0.9090909361839294, 0.75, 0.06593406945466995, 1.0, 0.3076923191547394, 1.0, 1.0, 0.5862069129943848, 0.6666666865348816, 0.5, 0.12389380484819412, 1.0, 1.0, 0.8666666746139526, 1.0, 0.7857142686843872, 0.8571428656578064, 0.8333333134651184, 0.10752688348293304, 0.5454545617103577, 0.20661157369613647, 1.0, 1.0, 1.0, 0.6666666865348816, 1.0, 1.0, 1.0, 1.0, 0.8333333134651184, 0.6666666865348816, 1.0, 0.3510638177394867, 1.0, 1.0, 1.0, 1.0, 0.6666666865348816, 0.6666666865348816, 1.0, 1.0, 1.0, 0.5, 0.6666666865348816, 1.0, 0.8999999761581421, 1.0, 1.0, 1.0, 0.3333333432674408, 0.7777777910232544, 1.0, 0.8888888955116272, 0.5, 0.7777777910232544, 1.0, 1.0, 0.04210526496171951, 1.0, 1.0, 0.25, 0.08396946638822556, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7333333492279053, 0.692307710647583, 1.0, 1.0, 0.692307710647583, 1.0, 0.75, 0.4000000059604645, 1.0, 0.8571428656578064, 0.75, 1.0, 0.6666666865348816, 1.0, 1.0, 1.0, 0.039603959769010544, 1.0, 0.7272727489471436, 0.800000011920929, 0.6470588445663452, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.800000011920929, 0.3333333432674408, 1.0, 1.0, 0.01923076994717121, 1.0, 0.800000011920929, 1.0, 0.5, 0.8461538553237915, 1.0, 1.0, 1.0, 1.0, 0.03333333507180214, 1.0, 0.06896551698446274, 0.018018018454313278, 0.01904761977493763, 0.5, 1.0, 0.75, 0.017241379246115685, 0.6666666865348816, 1.0, 0.75, 1.0, 1.0, 0.7727272510528564, 1.0, 0.6666666865348816, 0.5, 1.0, 1.0, 0.6875, 1.0, 1.0, 0.03921568766236305, 1.0, 0.24742268025875092, 0.02150537632405758, 1.0, 0.5714285969734192, 1.0, 1.0, 0.75, 1.0, 1.0, 0.01785714365541935, 1.0, 0.875, 1.0, 0.0746268630027771, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333432674408, 1.0, 0.5, 0.8181818127632141, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333134651184, 0.6666666865348816, 0.5, 1.0, 1.0, 0.5, 1.0, 0.06722689419984818, 0.42424243688583374, 1.0, 1.0, 0.0982142835855484, 1.0, 0.6666666865348816, 1.0, 0.9230769276618958, 0.5, 1.0, 0.3913043439388275, 1.0, 0.3571428656578064, 1.0, 1.0, 1.0, 1.0, 0.15447154641151428]
simple_rank_outputs = [0.75, 0.4444444477558136, 1.0, 1.0, 1.0, 0.75, 0.3448275923728943, 0.8235294222831726, 0.6000000238418579, 0.800000011920929, 0.5, 1.0, 0.529411792755127, 0.8333333134651184, 0.5, 1.0, 0.800000011920929, 1.0, 1.0, 1.0, 0.5, 1.0, 0.04040404036641121, 1.0, 1.0, 1.0, 0.03883495181798935, 1.0, 1.0, 1.0, 0.06521739065647125, 0.8260869383811951, 1.0, 0.01785714365541935, 1.0, 1.0, 1.0, 0.5, 0.5, 0.6666666865348816, 0.5, 0.75, 1.0, 1.0, 0.03846153989434242, 1.0, 0.75, 1.0, 0.8181818127632141, 1.0, 1.0, 0.5555555820465088, 1.0, 0.6666666865348816, 0.800000011920929, 1.0, 1.0, 0.10948905348777771, 0.25974026322364807, 1.0, 1.0, 0.5882353186607361, 0.8181818127632141, 1.0, 1.0, 0.75, 1.0, 0.06451612710952759, 1.0, 0.033898305147886276, 1.0, 0.5, 0.33636364340782166, 1.0, 0.5, 1.0, 0.3564356565475464, 0.75, 1.0, 0.07619047909975052, 0.800000011920929, 1.0, 1.0, 0.7857142686843872, 0.2525252401828766, 0.6666666865348816, 1.0, 0.5, 0.8333333134651184, 1.0, 1.0, 1.0, 0.12264151126146317, 1.0, 1.0, 1.0, 0.625, 1.0, 0.5, 1.0, 0.7777777910232544, 0.3333333432674408, 0.75, 0.3333333432674408, 0.875, 0.4444444477558136, 0.5, 0.5833333134651184, 1.0, 0.625, 0.08791209012269974, 1.0, 1.0, 0.1489361673593521, 0.7272727489471436, 1.0, 1.0, 1.0, 0.6666666865348816, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3461538553237915, 1.0, 0.8148148059844971, 1.0, 1.0, 1.0, 0.25, 0.018691588193178177, 0.7142857313156128, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7142857313156128, 0.6000000238418579, 1.0, 1.0, 1.0, 1.0, 0.5714285969734192, 0.6666666865348816, 0.5185185074806213, 1.0, 0.6666666865348816, 0.8333333134651184, 1.0, 0.25, 0.8461538553237915, 1.0, 0.5, 1.0, 1.0, 0.7083333134651184, 1.0, 1.0, 0.4615384638309479, 1.0, 0.5, 0.6363636255264282, 1.0, 0.8333333134651184, 0.5, 1.0, 0.4615384638309479, 0.5151515007019043, 1.0, 0.7200000286102295, 1.0, 0.5, 1.0, 0.78125, 1.0, 0.6666666865348816, 1.0, 1.0, 0.6666666865348816, 1.0, 0.6666666865348816, 1.0, 1.0, 0.6666666865348816, 1.0, 0.020618556067347527, 0.12931033968925476, 0.5, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.800000011920929, 0.5, 1.0, 0.03669724613428116, 0.6666666865348816, 0.5909090638160706, 0.6363636255264282, 0.5714285969734192, 0.5, 1.0, 1.0, 1.0, 1.0, 0.4375, 1.0, 1.0, 0.4000000059604645, 1.0, 0.016260161995887756, 0.5, 0.5714285969734192, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.5384615659713745, 1.0, 0.15315315127372742, 1.0, 1.0, 1.0, 0.5, 1.0, 0.75, 0.25, 1.0, 1.0, 0.06451612710952759, 0.5, 1.0, 0.3333333432674408, 0.5333333611488342, 0.75, 1.0, 0.19298245012760162, 0.6666666865348816, 1.0, 1.0, 1.0, 1.0, 0.2666666805744171, 0.7333333492279053, 1.0, 0.6666666865348816, 0.8888888955116272, 0.7142857313156128, 0.2222222238779068, 0.6666666865348816, 0.6666666865348816, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.13333334028720856, 0.6842105388641357, 0.875, 1.0, 1.0, 1.0, 0.4000000059604645, 0.5, 0.6666666865348816, 0.75, 1.0, 0.03773584961891174, 1.0, 1.0, 1.0, 0.6666666865348816, 1.0, 1.0, 0.6666666865348816, 0.6000000238418579, 1.0, 1.0, 1.0, 0.059405941516160965, 0.8333333134651184, 1.0, 0.0181818176060915, 0.36666667461395264, 1.0, 1.0, 1.0, 0.4285714328289032, 1.0, 0.16239316761493683, 0.75, 1.0, 0.8333333134651184, 0.3804347813129425, 1.0, 0.06593406945466995, 1.0, 0.5, 0.7692307829856873, 1.0, 0.529411792755127, 0.2110091745853424, 1.0, 1.0, 1.0, 1.0, 0.7368420958518982, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666865348816, 1.0, 0.019999999552965164, 0.75, 0.019999999552965164, 0.020202020183205605, 0.06976744532585144, 1.0, 0.75, 1.0, 1.0, 0.8333333134651184, 0.699999988079071, 0.03883495181798935, 1.0, 1.0, 0.3333333432674408, 1.0, 1.0, 1.0, 0.5, 1.0, 0.3457943797111511, 1.0, 0.692307710647583, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7142857313156128, 1.0, 1.0, 0.1484375, 0.75, 1.0, 0.3636363744735718, 0.7272727489471436, 0.75, 0.375, 1.0, 1.0, 1.0, 0.5789473652839661, 0.7142857313156128, 0.12621359527111053, 1.0, 0.6363636255264282, 0.6000000238418579, 1.0, 1.0, 0.75, 0.6842105388641357, 0.6000000238418579, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 0.020408162847161293, 1.0, 0.7631579041481018, 0.6000000238418579, 1.0, 1.0, 0.7647058963775635, 1.0, 0.6315789222717285, 0.5, 0.75, 0.6000000238418579, 1.0, 1.0, 1.0, 1.0, 0.3333333432674408, 0.5, 0.5, 1.0, 0.1666666716337204, 1.0, 1.0, 1.0, 1.0, 0.800000011920929, 1.0, 0.6000000238418579, 1.0, 1.0, 0.20512820780277252, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6000000238418579, 0.7777777910232544, 1.0, 1.0, 0.06185567006468773, 1.0, 1.0, 0.8333333134651184, 1.0, 1.0, 1.0, 0.3333333432674408, 0.5, 0.75, 1.0, 1.0, 0.75, 1.0, 0.800000011920929, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7272727489471436, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5887850522994995, 1.0, 0.7142857313156128, 0.75, 1.0, 1.0, 1.0, 0.7272727489471436, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.3333333432674408, 0.75, 0.6666666865348816, 0.16296295821666718, 1.0, 1.0, 0.01785714365541935, 1.0, 1.0, 1.0, 1.0, 0.8571428656578064, 1.0, 0.4736842215061188, 1.0, 0.3333333432674408, 1.0, 0.3333333432674408, 1.0, 0.8333333134651184, 1.0, 0.6000000238418579, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.800000011920929, 1.0, 1.0, 1.0, 0.5, 0.5, 0.6000000238418579, 0.7058823704719543, 1.0, 1.0, 1.0, 0.6399999856948853, 1.0, 1.0, 0.8823529481887817, 1.0, 0.6666666865348816, 1.0, 1.0, 0.800000011920929, 0.1034482792019844, 1.0, 1.0, 1.0, 0.523809552192688, 1.0, 1.0, 0.7142857313156128, 0.05833333358168602, 1.0, 0.75, 1.0, 0.4399999976158142, 1.0, 1.0, 1.0, 0.6666666865348816, 0.800000011920929, 1.0, 1.0, 1.0, 0.692307710647583, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6363636255264282, 0.5, 0.5, 0.8500000238418579, 0.8333333134651184, 0.06818182021379471, 0.5, 1.0, 0.7222222089767456, 1.0, 0.3333333432674408, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.5555555820465088, 1.0, 0.13483145833015442, 1.0, 0.3333333432674408, 1.0, 1.0, 1.0, 0.04210526496171951, 1.0, 1.0, 1.0, 0.11235955357551575, 0.5, 0.6666666865348816, 0.5833333134651184, 0.5, 0.6000000238418579, 1.0, 0.6000000238418579, 0.10204081982374191, 1.0, 0.4285714328289032, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333134651184, 0.6000000238418579, 1.0, 0.6428571343421936, 1.0, 0.6666666865348816, 0.017241379246115685, 1.0, 0.8888888955116272, 0.6000000238418579, 0.523809552192688, 0.7647058963775635, 1.0, 0.6000000238418579, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.5, 0.8888888955116272, 0.6666666865348816, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666865348816, 0.1355932205915451, 0.75, 0.75, 1.0, 1.0, 1.0, 1.0, 0.2380952388048172, 1.0, 0.5, 0.75, 0.6666666865348816, 1.0, 0.2380952388048172, 0.5, 0.6666666865348816, 0.5, 0.75, 1.0, 1.0, 1.0, 1.0, 0.6666666865348816, 0.8181818127632141, 1.0, 0.6000000238418579, 1.0, 1.0, 1.0, 1.0, 0.5, 0.7142857313156128, 1.0, 0.6363636255264282, 0.5625, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666865348816, 0.5, 0.4000000059604645, 1.0, 0.75, 0.6666666865348816, 1.0, 0.07999999821186066, 0.6666666865348816, 0.5, 1.0, 1.0, 0.6666666865348816, 0.6470588445663452, 1.0, 1.0, 1.0, 0.692307710647583, 0.6666666865348816, 1.0, 1.0, 1.0, 0.6666666865348816, 0.6666666865348816, 0.875, 0.7647058963775635, 0.6666666865348816, 0.6666666865348816, 1.0, 0.3333333432674408, 0.5714285969734192, 1.0, 0.03361344709992409, 1.0, 0.040816325694322586, 0.11999999731779099, 1.0, 1.0, 0.21374045312404633, 0.018518518656492233, 0.8461538553237915, 0.0416666679084301, 0.75, 1.0, 0.75, 0.36283186078071594, 0.5, 1.0, 1.0, 1.0, 0.17543859779834747, 0.039603959769010544, 0.7916666865348816, 0.4000000059604645, 0.5, 1.0, 0.800000011920929, 1.0, 0.22033898532390594, 0.6666666865348816, 0.7037037014961243, 0.019801979884505272, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6315789222717285, 0.75, 0.0941176488995552, 1.0, 0.10958904027938843, 0.5, 0.9047619104385376, 1.0, 1.0, 0.7142857313156128, 1.0, 1.0, 0.6666666865348816, 0.6000000238418579, 0.9090909361839294, 0.75, 0.06593406945466995, 1.0, 0.3076923191547394, 1.0, 1.0, 0.5862069129943848, 0.6666666865348816, 0.5, 0.1592920422554016, 1.0, 1.0, 0.8666666746139526, 1.0, 0.7857142686843872, 0.8571428656578064, 0.8333333134651184, 0.10752688348293304, 0.5454545617103577, 0.20661157369613647, 1.0, 1.0, 1.0, 0.7777777910232544, 1.0, 1.0, 1.0, 1.0, 0.8333333134651184, 0.6666666865348816, 1.0, 0.3510638177394867, 1.0, 1.0, 1.0, 1.0, 0.6666666865348816, 0.6666666865348816, 1.0, 1.0, 1.0, 0.5, 0.6666666865348816, 0.875, 0.8999999761581421, 1.0, 1.0, 1.0, 0.3333333432674408, 0.7777777910232544, 1.0, 0.8888888955116272, 0.5, 0.7777777910232544, 1.0, 1.0, 0.04210526496171951, 1.0, 1.0, 0.25, 0.08396946638822556, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7333333492279053, 0.692307710647583, 1.0, 1.0, 0.692307710647583, 1.0, 0.75, 0.6000000238418579, 1.0, 0.8571428656578064, 0.75, 0.75, 0.6666666865348816, 1.0, 1.0, 1.0, 0.039603959769010544, 1.0, 0.5454545617103577, 0.800000011920929, 0.6470588445663452, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.800000011920929, 0.3333333432674408, 1.0, 1.0, 0.01923076994717121, 1.0, 0.800000011920929, 1.0, 0.5, 0.8461538553237915, 1.0, 1.0, 1.0, 1.0, 0.03333333507180214, 1.0, 0.06896551698446274, 0.018018018454313278, 0.01904761977493763, 0.5, 1.0, 0.75, 0.017241379246115685, 0.6666666865348816, 1.0, 0.75, 1.0, 1.0, 0.7727272510528564, 1.0, 0.6666666865348816, 0.5, 1.0, 0.3333333432674408, 1.0, 1.0, 1.0, 0.03921568766236305, 1.0, 0.24742268025875092, 0.02150537632405758, 1.0, 0.761904776096344, 1.0, 1.0, 0.75, 1.0, 1.0, 0.0357142873108387, 0.75, 0.875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333432674408, 1.0, 0.5, 0.8181818127632141, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333134651184, 0.6666666865348816, 0.5, 1.0, 1.0, 0.5, 1.0, 0.06722689419984818, 0.42424243688583374, 1.0, 1.0, 0.0982142835855484, 1.0, 0.6666666865348816, 1.0, 0.9230769276618958, 0.5, 1.0, 0.3913043439388275, 1.0, 0.3571428656578064, 1.0, 1.0, 1.0, 1.0, 1.0]
gcn_rank_outputs = [0.75, 0.4444444477558136, 1.0, 1.0, 1.0, 0.875, 0.09195402264595032, 0.8235294222831726, 0.6000000238418579, 0.800000011920929, 0.5, 1.0, 0.4117647111415863, 0.8333333134651184, 0.5, 1.0, 0.6000000238418579, 0.5, 1.0, 0.4579439163208008, 0.30000001192092896, 0.25, 0.020202020183205605, 0.00909090880304575, 1.0, 1.0, 0.03883495181798935, 1.0, 1.0, 1.0, 0.06521739065647125, 0.043478261679410934, 1.0, 0.01785714365541935, 1.0, 0.8571428656578064, 1.0, 0.5, 0.5, 0.3333333432674408, 0.5, 0.75, 1.0, 1.0, 0.03846153989434242, 0.239130437374115, 0.75, 0.04385964944958687, 0.5454545617103577, 1.0, 1.0, 0.5555555820465088, 0.25, 0.6666666865348816, 0.800000011920929, 1.0, 1.0, 0.10218977928161621, 0.25974026322364807, 1.0, 1.0, 0.8235294222831726, 0.5, 1.0, 1.0, 0.5, 1.0, 0.02150537632405758, 0.009523809887468815, 0.033898305147886276, 1.0, 0.25, 0.3272727131843567, 1.0, 0.5, 1.0, 0.3663366436958313, 0.75, 1.0, 0.07619047909975052, 0.800000011920929, 1.0, 1.0, 0.5357142686843872, 0.14141413569450378, 0.6666666865348816, 0.3529411852359772, 0.5, 0.8333333134651184, 1.0, 1.0, 0.09482758492231369, 0.09433962404727936, 0.5, 1.0, 1.0, 0.625, 1.0, 0.25, 0.5, 0.7222222089767456, 0.3333333432674408, 0.75, 0.3333333432674408, 0.6875, 0.5555555820465088, 0.5, 0.5833333134651184, 1.0, 0.625, 0.08791209012269974, 1.0, 1.0, 1.0, 0.5454545617103577, 1.0, 1.0, 1.0, 0.5, 1.0, 0.1651376187801361, 0.22857142984867096, 1.0, 1.0, 0.3461538553237915, 1.0, 0.7037037014961243, 1.0, 1.0, 1.0, 0.25, 0.018691588193178177, 0.4285714328289032, 0.5714285969734192, 1.0, 1.0, 1.0, 0.31313130259513855, 0.7142857313156128, 0.4000000059604645, 1.0, 1.0, 0.020618556067347527, 1.0, 0.5714285969734192, 0.6666666865348816, 0.03703703731298447, 1.0, 0.6666666865348816, 0.8333333134651184, 1.0, 0.25, 0.8461538553237915, 1.0, 0.5, 1.0, 1.0, 0.1666666716337204, 1.0, 0.23893804848194122, 0.4615384638309479, 1.0, 0.5, 0.4545454680919647, 1.0, 0.8333333134651184, 0.5, 1.0, 0.5384615659713745, 0.4848484992980957, 1.0, 0.36000001430511475, 1.0, 0.5, 1.0, 0.6875, 0.1607142835855484, 0.3333333432674408, 1.0, 1.0, 0.6666666865348816, 0.2734375, 0.3333333432674408, 0.5, 1.0, 0.6666666865348816, 0.5398229956626892, 0.020618556067347527, 0.23275862634181976, 0.5, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.800000011920929, 0.5, 1.0, 0.027522936463356018, 0.3333333432674408, 0.4545454680919647, 0.7272727489471436, 0.1428571492433548, 0.5, 1.0, 0.009708737954497337, 1.0, 1.0, 0.375, 0.6095238327980042, 1.0, 0.4000000059604645, 0.2300885021686554, 0.016260161995887756, 0.5, 0.7142857313156128, 1.0, 0.1735537201166153, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.4615384638309479, 1.0, 0.12612612545490265, 1.0, 0.75, 1.0, 0.5, 1.0, 0.800000011920929, 0.25, 1.0, 1.0, 0.06451612710952759, 0.5, 1.0, 0.3333333432674408, 0.5333333611488342, 0.75, 1.0, 0.1666666716337204, 0.4444444477558136, 0.1666666716337204, 0.0517241396009922, 1.0, 1.0, 0.2666666805744171, 0.6666666865348816, 1.0, 0.6666666865348816, 0.8888888955116272, 0.4285714328289032, 0.20512820780277252, 0.6666666865348816, 0.3333333432674408, 1.0, 0.07017543911933899, 1.0, 1.0, 0.5, 1.0, 0.6000000238418579, 1.0, 1.0, 1.0, 1.0, 0.1111111119389534, 0.6315789222717285, 0.75, 0.800000011920929, 1.0, 1.0, 0.20000000298023224, 0.5, 0.5, 0.5, 1.0, 0.01886792480945587, 1.0, 1.0, 0.24817518889904022, 0.6666666865348816, 1.0, 1.0, 0.6666666865348816, 0.6000000238418579, 1.0, 1.0, 0.06194690242409706, 0.039603959769010544, 0.1666666716337204, 1.0, 0.0181818176060915, 0.4000000059604645, 1.0, 1.0, 0.5, 1.0, 1.0, 0.17094017565250397, 0.75, 1.0, 0.9166666865348816, 0.30434781312942505, 1.0, 0.04395604506134987, 1.0, 0.5, 0.6153846383094788, 1.0, 0.8235294222831726, 0.15596330165863037, 1.0, 0.27826085686683655, 1.0, 1.0, 0.7368420958518982, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666865348816, 0.20454545319080353, 0.019999999552965164, 0.25, 0.019999999552965164, 0.020202020183205605, 0.04651162773370743, 1.0, 0.875, 1.0, 1.0, 0.6666666865348816, 0.800000011920929, 0.03883495181798935, 1.0, 1.0, 0.3333333432674408, 1.0, 1.0, 1.0, 0.5, 1.0, 0.2429906576871872, 1.0, 0.8461538553237915, 1.0, 0.7777777910232544, 1.0, 0.09615384787321091, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7142857313156128, 1.0, 1.0, 1.0, 0.75, 1.0, 0.30909091234207153, 1.0, 1.0, 0.375, 1.0, 1.0, 1.0, 0.5263158082962036, 0.7142857313156128, 1.0, 0.2053571492433548, 0.6363636255264282, 0.699999988079071, 1.0, 1.0, 0.75, 0.6842105388641357, 0.4000000059604645, 1.0, 1.0, 1.0, 0.1875, 1.0, 0.0833333358168602, 1.0, 1.0, 0.020408162847161293, 1.0, 0.4736842215061188, 0.4000000059604645, 1.0, 0.1827957034111023, 0.7647058963775635, 0.13793103396892548, 0.6842105388641357, 0.5, 0.25, 0.550000011920929, 1.0, 1.0, 1.0, 1.0, 0.3333333432674408, 0.1666666716337204, 0.5, 1.0, 0.1388888955116272, 1.0, 1.0, 1.0, 1.0, 0.4000000059604645, 1.0, 0.6000000238418579, 0.5, 1.0, 0.18803419172763824, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6000000238418579, 0.4444444477558136, 0.71875, 1.0, 0.04123711213469505, 1.0, 1.0, 0.6666666865348816, 0.01834862306714058, 1.0, 0.3730158805847168, 0.3333333432674408, 0.25, 0.75, 1.0, 1.0, 0.9375, 1.0, 0.800000011920929, 1.0, 1.0, 0.5, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 0.7272727489471436, 0.10084033757448196, 1.0, 1.0, 1.0, 0.5, 0.6111111044883728, 0.5514018535614014, 1.0, 0.7142857313156128, 0.75, 1.0, 1.0, 1.0, 0.7272727489471436, 1.0, 1.0, 0.1818181872367859, 1.0, 1.0, 1.0, 0.34951457381248474, 1.0, 1.0, 1.0, 0.5, 0.5555555820465088, 0.75, 1.0, 0.14814814925193787, 1.0, 1.0, 0.01785714365541935, 1.0, 1.0, 1.0, 1.0, 0.7142857313156128, 0.7727272510528564, 0.7894737124443054, 1.0, 0.3333333432674408, 1.0, 0.3333333432674408, 1.0, 0.8333333134651184, 1.0, 0.6000000238418579, 0.0923076942563057, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 0.699999988079071, 1.0, 1.0, 0.7368420958518982, 0.5, 0.5, 0.6000000238418579, 0.5882353186607361, 1.0, 1.0, 0.1796875, 0.5199999809265137, 1.0, 1.0, 0.6470588445663452, 1.0, 0.6666666865348816, 1.0, 0.008064515888690948, 0.6000000238418579, 0.08620689809322357, 1.0, 1.0, 1.0, 0.2857142984867096, 1.0, 0.6666666865348816, 0.7142857313156128, 1.0, 1.0, 0.125, 0.032967034727334976, 0.47999998927116394, 1.0, 1.0, 1.0, 1.0, 0.6000000238418579, 1.0, 1.0, 1.0, 0.07692307978868484, 1.0, 0.13274335861206055, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6363636255264282, 0.5, 0.5, 0.699999988079071, 0.8333333134651184, 0.06818182021379471, 0.5, 1.0, 0.6111111044883728, 1.0, 0.3333333432674408, 1.0, 1.0, 0.625, 1.0, 0.39449542760849, 1.0, 0.6666666865348816, 1.0, 0.04494382068514824, 1.0, 0.6666666865348816, 1.0, 0.1964285671710968, 0.21153846383094788, 0.04210526496171951, 1.0, 1.0, 0.1111111119389534, 0.11235955357551575, 0.5, 0.6666666865348816, 0.5, 0.5, 0.5, 1.0, 0.6000000238418579, 0.10204081982374191, 1.0, 0.5714285969734192, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.1946902722120285, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333134651184, 0.800000011920929, 1.0, 0.5714285969734192, 1.0, 0.6666666865348816, 0.017241379246115685, 1.0, 0.6666666865348816, 0.4000000059604645, 0.8571428656578064, 0.5882353186607361, 0.017094017937779427, 0.6000000238418579, 0.3333333432674408, 1.0, 0.5, 1.0, 1.0, 1.0, 0.4375, 1.0, 1.0, 0.5, 0.5555555820465088, 0.5555555820465088, 0.5, 0.009803921915590763, 0.25, 1.0, 0.875, 1.0, 1.0, 1.0, 0.6666666865348816, 0.12711864709854126, 0.5, 0.550000011920929, 1.0, 1.0, 1.0, 1.0, 0.15238095819950104, 0.20909090340137482, 0.5, 0.75, 0.6666666865348816, 1.0, 0.190476194024086, 0.75, 0.6666666865348816, 0.5, 0.699999988079071, 1.0, 1.0, 0.1090909093618393, 1.0, 0.6666666865348816, 0.8181818127632141, 1.0, 0.800000011920929, 1.0, 1.0, 1.0, 1.0, 0.6666666865348816, 0.1428571492433548, 1.0, 0.3636363744735718, 0.5625, 0.5, 1.0, 0.25, 0.1304347813129425, 1.0, 0.6666666865348816, 1.0, 1.0, 0.6666666865348816, 0.5, 0.4000000059604645, 1.0, 0.75, 0.3333333432674408, 1.0, 0.05999999865889549, 0.3333333432674408, 0.5, 1.0, 1.0, 0.6666666865348816, 0.5882353186607361, 1.0, 1.0, 1.0, 0.5384615659713745, 0.6666666865348816, 1.0, 1.0, 1.0, 0.6666666865348816, 1.0, 0.625, 0.7058823704719543, 0.6666666865348816, 0.6666666865348816, 1.0, 0.3333333432674408, 0.4642857015132904, 1.0, 0.03361344709992409, 1.0, 0.020408162847161293, 0.11999999731779099, 1.0, 1.0, 1.0, 0.018518518656492233, 0.692307710647583, 0.02083333395421505, 0.5, 1.0, 0.75, 0.3362831771373749, 0.5, 1.0, 0.11428571492433548, 1.0, 1.0, 0.039603959769010544, 0.0833333358168602, 0.4000000059604645, 0.5, 1.0, 0.6000000238418579, 1.0, 0.12711864709854126, 0.6666666865348816, 0.5185185074806213, 0.019801979884505272, 1.0, 0.75, 1.0, 1.0, 0.01600000075995922, 0.5789473652839661, 0.675000011920929, 0.0941176488995552, 1.0, 0.10958904027938843, 0.5, 0.5714285969734192, 1.0, 1.0, 0.4285714328289032, 1.0, 1.0, 0.5, 0.800000011920929, 0.7272727489471436, 0.0416666679084301, 0.04395604506134987, 1.0, 0.692307710647583, 1.0, 1.0, 0.5517241358757019, 0.6666666865348816, 0.5, 0.12389380484819412, 1.0, 1.0, 0.5333333611488342, 1.0, 0.7142857313156128, 0.7142857313156128, 0.3333333432674408, 0.06451612710952759, 0.5454545617103577, 0.1735537201166153, 1.0, 0.37254902720451355, 1.0, 0.4444444477558136, 1.0, 1.0, 1.0, 0.38461539149284363, 0.8333333134651184, 1.0, 0.800000011920929, 0.3723404109477997, 1.0, 1.0, 1.0, 0.2460317462682724, 0.6666666865348816, 0.6666666865348816, 1.0, 1.0, 0.800000011920929, 0.5, 0.6666666865348816, 0.875, 0.6000000238418579, 1.0, 1.0, 1.0, 0.3333333432674408, 0.7777777910232544, 0.24778760969638824, 0.8888888955116272, 0.5, 0.6666666865348816, 1.0, 1.0, 0.04210526496171951, 1.0, 1.0, 0.1666666716337204, 0.09923664480447769, 1.0, 1.0, 1.0, 1.0, 0.2358490526676178, 0.6666666865348816, 0.23076923191547394, 0.06930693238973618, 1.0, 0.6153846383094788, 1.0, 0.75, 0.20000000298023224, 1.0, 0.7142857313156128, 0.75, 0.625, 0.6666666865348816, 1.0, 1.0, 1.0, 0.039603959769010544, 1.0, 0.6363636255264282, 0.800000011920929, 0.47058823704719543, 1.0, 0.8333333134651184, 1.0, 0.27272728085517883, 0.13178294897079468, 1.0, 0.6000000238418579, 0.3333333432674408, 0.008064515888690948, 1.0, 0.01923076994717121, 1.0, 0.800000011920929, 1.0, 0.5, 0.7692307829856873, 1.0, 1.0, 1.0, 1.0, 0.03333333507180214, 1.0, 0.09195402264595032, 0.018018018454313278, 0.01904761977493763, 0.5, 1.0, 0.5, 0.017241379246115685, 0.6666666865348816, 1.0, 0.75, 1.0, 1.0, 0.04545454680919647, 1.0, 0.6666666865348816, 0.5, 1.0, 0.3333333432674408, 0.625, 1.0, 1.0, 0.05882352963089943, 1.0, 0.22680412232875824, 0.02150537632405758, 1.0, 0.6666666865348816, 1.0, 1.0, 0.625, 1.0, 1.0, 0.01785714365541935, 0.75, 0.125, 1.0, 0.04477611929178238, 1.0, 1.0, 0.02654867246747017, 1.0, 1.0, 1.0, 0.3333333432674408, 1.0, 0.5, 0.8181818127632141, 0.040816325694322586, 0.19607843458652496, 1.0, 1.0, 0.009345794096589088, 1.0, 0.5833333134651184, 0.3333333432674408, 0.5, 1.0, 1.0, 0.5, 1.0, 0.016806723549962044, 0.3636363744735718, 1.0, 1.0, 0.0625, 1.0, 0.6666666865348816, 1.0, 0.692307710647583, 1.0, 1.0, 0.3478260934352875, 1.0, 0.2857142984867096, 1.0, 1.0, 0.13461539149284363, 0.09836065769195557, 0.13821138441562653]

#0.7807004628886367 bc
#0.7793166646531209 simple rank
#0.6855881478318974 pairwise